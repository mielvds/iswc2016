ID,NR,Authors,Lastname,Title,Track,Session,PIndex,Presenter,file,Room,Start,End,Abstract
demo_48,48,Jędrzej Potoniec and Agnieszka Ławrynowicz,Potoniec,A Protege Plugin with Swift Linked Data Miner,demo,Poster and Demo session,1,Jędrzej Potoniec,demo_48.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We present a Protege plugin implementing Swift Linked Data Miner, an anytime algorithm for extending an ontology with new subsumptions. The algorithm mines an RDF graph accessible via a SPARQL endpoint and proposes new SubClassOf axioms to the user."
demo_114,114,"Pasquale Lisena, Manel Achichi, Eva Fernandez, Konstantin Todorov and Raphaël Troncy",Lisena,Exploring Linked Classical Music Catalogs with OVERTURE,demo,Poster and Demo session,2,Pasquale Lisena,demo_114.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper, we introduce OVERTURE - a web application allowing to explore the interlinked catalogs of major music libraries including the French National Library, Radio France and the Philharmonie de Paris. We have first developed the DOREMUS ontology which is an extension of the well-known FRBRoo model for describing works and expressions as well as the creation processus. We have implemented a so-called marc2rdf tool allowing for the conversion and linking of bibliographical entries about music works, interpretations and expressions from their original MARC-format to RDF following this DOREMUS ontology. We present an exploratory search engine prototype that enables to browse through the reconciled collection of bibliographical records of classical music and to highlight the various interpretations of a work, its derivative, its performance casting as well as other rich metadata."
poster_23,23,John P. Mccrae and Philipp Cimiano,Mccrae,"LIXR: Quick, succinct conversion of XML to RDF",poster,Poster and Demo session,25,John P. Mccrae,poster_23.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"This paper presents LIXR, a system for converting between RDF and XML. LIXR is based on domain-specific language embedded into the Scala programming language. It supports the definition of transformations of datasets from RDF to XML in a declarative fashion, while still maintaining the flexibility of a full programming language environment. We directly compare this system to other systems programmed in Java and XSLT and show that the LIXR implementations are significantly shorter in terms of lines of code, in addition to being bidirectional and conceptually simple to understand.
"
demo_116,116,"Konstantina Bereta, Guohui Xiao, Manolis Koubarakis, Martina Hodrius and Conrad Bielski",Bereta,Ontop-spatial for Geospatial Data Integration using GeoSPARQL-to-SQL Translation,demo,Poster and Demo session,4,Konstantina Bereta,demo_116.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We present Ontop-spatial, a geospatial extension of the well-known OBDA system Ontop, that leverages the technologies of geospatial databases and enables GeoSPARQL-to-SQL translation. We showcase the functionalities of the system in real-world use cases which require data integration of different geospatial sources."
demo_42,42,"Nandana Mihindukulasooriya, Esteban Gonzalez, Fernando Serena, Carlos Badenes and Oscar Corcho",Mihindukulasooriya,FarolApp: Live Linked Data on Light Pollution,demo,Poster and Demo session,5,Nandana Mihindukulasooriya,demo_42.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"FarolApp is a mobile web application that aims to increase the awareness of light pollution by generating illustrative maps for cities and by encouraging citizens and public administrations to provide street light information in an ubiquitous and interactive way using online street views. In addition to the maps, FarolApp builds on existing sources to generate and provide up-to-date data by crowdsourced user annotations. Generated data is available as dereferenceable Linked Data resources in several RDF formats and via a queryable SPARQL endpoint. The demo presented in this paper illustrates how FarolApp maintains continuously evolving Linked Data that reflect the current status of city street light infrastructures and use that data to generate light pollution maps."
demo_47,47,"Alo Allik, Mariano Mora-Mcginity, Gyorgy Fazekas and Mark Sandler",Allik,MusicWeb: music discovery with open linked semantic metadata,demo,Poster and Demo session,6,Alo Allik,demo_47.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"This demo presents MusicWeb, a novel platform for linking music artists within a web-based application for discovering associations between them. MusicWeb provides a browsing experience using connections that are either extra-musical or tangential to music, such as the artists' political affiliation or social influence, or intra-musical, such as the artists' main instrument or most favoured musical key. The platform integrates open linked semantic metadata from various Semantic Web, music recommendation and social media data sources. The connections are further supplemented by thematic analysis of journal articles, blog posts and content-based similarity measures focussing on high level musical categories."
demo_113,113,"Jacopo Urbani, Ceriel Jacobs and Markus Krötzsch",Urbani,VLog: A Column-Oriented Datalog System for Large Knowledge Graphs,demo,Poster and Demo session,7,Jacopo Urbani,demo_113.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We present VLog, a new system for answering arbitrary Datalog queries on top of a wide range of databases, including both relational and RDF databases. VLog is designed to perform efficiently intensive rule-based computation on large Knowledge Graphs (KGs). It adapts column-store technologies to attain high efficiency in terms of memory usage and speed, enabling us to process Datalog queries with thousands of rules over databases with hundreds of millions of tuples---in a live demonstration on a laptop. Our demonstration provides in-depth insights into the workings of VLog, and presents important new features such as support for arbitrary relational DBMS."
poster_81,81,Katalin Ternai and Ildikó Szabó,Ternai,Semantic Audit Application,poster,Poster and Demo session,8,Katalin Ternai,poster_81.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Compliance checking of business processes executed by auditors requires to analyze documents e.g. log files, business process models depending on requirements derived from reference guidelines. This paper presents a forward compliance checking application for facilitating conformant behavior by de-tecting organizational operations and their deviations based on these docu-ments in a semantic way. This application has been tested on the Internaliza-tion process in the respect of Erasmus mobility."
poster_82,82,"Davide Lanti, Guohui Xiao and Diego Calvanese",Lanti,An Evaluation of VIG with the BSBM Benchmark,poster,Poster and Demo session,9,Davide Lanti,poster_82.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper we present an experimental evaluation of VIG, a data scaler for OBDA benchmarks. Data scaling is a relatively recent approach, proposed in the database community, that allows for quickly scaling an input data instance to s times its size, while preserving certain application-specific characteristics. The advantages of scaling are that the generator is general, in the sense that it can be re-used on different database schemas, and that users are not required to manually input the data characteristics. VIG lifts the scaling approach from the database level to the OBDA level, where the domain information of ontologies and mappings has to be taken into account as well.
 To evaluate the quality of VIG, in this paper we use it to generate data for the Berlin SPARQL Benchmark (BSBM), and compare it with the official BSBM data generator."
poster_85,85,"Seungjun Yoon, Sejin Chun, Xiongnan Jin and Kyong-Ho Lee",Yoon,A Unified Interface for Optimizing Continuous Query in Heterogeneous RDF Stream Processing Systems,poster,Poster and Demo session,10,Seungjun Yoon,poster_85.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The W3C RDF Stream Processing (RSP) community has proposed both a common model and a language for querying RDF streams. However, the current implementations of RSP systems are significantly different from each other in terms of performance. In this paper, we propose a unified interface for optimizing a continuous query in heterogeneous RSP systems. To enhance the performance of RSP, the unified interface decomposes query, reassembles partial queries and assigns them to appropriate RSP systems. Experimental results show that the proposed approach performances better in terms of memory consumption and latency."
poster_86,86,"Silvio Peroni, David Shotton and Fabio Vitali",Peroni,Jailbreaking your reference lists: the OpenCitations strike again,poster,Poster and Demo session,11,Silvio Peroni,poster_86.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this poster paper we provide an overview of the OpenCitations project and of its main outcome, the OpenCitations Corpus, which is an open repository of scholarly citation data made available under a Creative Commons public domain dedication, which provides in RDF accurate citation information harvested from the scholarly literature."
poster_8,8,Jing Mei,Mei,Building Evidence Graph for Clinical Decision Support,poster,Poster and Demo session,27,Jing Mei,poster_8.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Evidence-based medicine intends to optimize clinical decision making by using evidence. Semantic query answering could help to find the most relevant evidence. However, at point of care, it still lacks time for human reading of the evidence. In this poster, we propose to build an evidence graph for clinical decision support, in which an evidence ontology is defined with extension of SWRL rules. On top of this graph, we do evidence query and evidence fusion to generate the ranking list of decision options. Our prototype implementation of the evidence graph demonstrates its assistance to decision making, by combining a variety of knowledge-driven and data-driven decision services."
resource_23,23,"Wei Hu, Haoxuan Li, Zequn Sun, Xinqi Qian, Lingkun Xue, Ermei Cao and Yuzhong Qu",Hu,Clinga: Bringing Chinese Physical and Human Geography in Linked Open Data,resource,Linked Data,4,Wei Hu,resource_23.pdf,504+505,2016-10-20T11:30:00,2016-10-20T11:50:00,"While the geographical domain has long been involved as an important part of the Linked Data, the small amount of Chinese linked geographical data hinders the integration and sharing of both Chinese and cross-lingual knowledge. In this paper, we contribute to the development of a new Chinese linked geographical dataset named Clinga, by obtaining data from the largest Chinese wiki encyclopedia. We manually design a new geography ontology to categorize a wide range of physical and human geographical entities, and carry out an automatic discovery of links to existing knowledge bases. The resulted Clinga dataset contains over half million Chinese geographical entities and is open access."
research_108,108,Gerard de Melo,Melo,WebBrain: Joint Neural Learning of Large-Scale Commonsense Knowledge,research,Embeddings & Neural Approaches,1,Gerard de Melo,research_108.pdf,502,2016-10-20T15:30:00,2016-10-20T15:50:00,"While massive volumes of text are now more easily available for knowledge harvesting, many important facts about our everyday world are not expressed in a particularly explicit way. To address this, we present WebBrain, a new approach for harvesting commonsense knowledge that relies on joint learning from Web-scale data to fill gaps in the knowledge acquisition. We train a neural network model that not only learns word2vec-style vector representations of words but also commonsense knowledge about them. This joint model allows general semantic information to aid in generalizing beyond the extracted commonsense relationships. Experiments show that we can obtain word embeddings that reflect word meanings, yet also allow us to capture conceptual relationships and commonsense knowledge about them."
resource_27,27,"Roberto Garcia, Rosa Gil, Juan Manuel Gimeno, Eirik Bakke and David Karger",Garcia,Benchmarking End-User Structured Data Search and Exploration,resource,Interaction,2,Roberto Garcia,resource_27.pdf,504+505,2016-10-19T11:20:00,2016-10-19T11:40:00,"The Semantic Web Community has invested significant research effort in developing systems for Semantic Web search and exploration. But while it has been easy to assess the systems' computational efficiency, it has been much harder to assess how well different semantic systems help their users find and browse information. In this article, we propose and demonstrate the use of a benchmark for evaluating them, similar to the TREC benchmark for evaluating traditional search engines. Our benchmark includes a set of typical user tasks and a well-defined procedure for assigning a measure of performance on those tasks to a semantic system. We demonstrate its application to one such system, Rhizomer. We intend for this work to initiate a community conversation that will lead to a general accepted framework for comparing systems and measuring, and thus encouraging, progress towards better semantic search and exploration tools."
resource_28,28,"Silvio Peroni, Giorgia Lodi, Luigi Asprino, Aldo Gangemi and Valentina Presutti",Peroni,FOOD: FOod in Open Data,resource,Linked Data,2,Silvio Peroni,resource_28.pdf,504+505,2016-10-20T10:50:00,2016-10-20T11:10:00,"This paper describes the outcome of an e-government project named FOOD, FOod in Open Data, which was carried out in the context of a collaboration between the Institute of Cognitive Sciences and Technologies of the Italian National Research Council, the Italian Ministry of Agriculture (MIPAAF) and the Italian Digital Agency (AgID). In particular, we implemented several ontologies for describing protected names of products (wine, pasta, fish, oil, etc.). In addition, we present the process carried out for producing and publishing a LOD dataset containing data extracted from existing Italian policy documents on such products and compliant with the aforementioned ontologies."
research_107,107,"Mohsen Taheriyan, Craig Knoblock, Pedro Szekely and José Luis Ambite",Taheriyan,Leveraging Linked Data to Discover Semantic Relations within Data Sources,research,Enriching Data Sources,2,Mohsen Taheriyan,research_107.pdf,502,2016-10-21T15:50:00,2016-10-21T16:10:00,"Mapping data to a shared domain ontology is a key step in publishing semantic content on the Web. Most of the work on automatically mapping structured and semi-structured sources to ontologies focuses on semantic labeling, i.e., annotating data fields with ontology classes and/or properties. However, a precise mapping that fully recovers the intended meaning of the data needs to describe the semantic relations between the data fields too. We present a novel approach to automatically discover the semantic relations within a given data source. We mine the small graph patterns occurring in Linked Open Data and combine them to build a graph that will be used to infer semantic relations. We evaluated our approach on datasets from different domains. Mining patterns of maximum length five, our method achieves an average precision of 75% and recall of 77% for a dataset with very complex mappings to the domain ontology, increasing up to 86% and 82%, respectively, for simpler ontologies and mappings."
demo_115,115,Wouter Beek and Jan Wielemaker,Beek,SWISH: An Integrated Semantic Web Notebook,demo,Poster and Demo session,3,Wouter Beek,demo_115.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,SPARQL editors make it easier to write and inspect their results. Notebooks already support computer- and data scientists in domains like statistics and machine learning. There is currently not an integrated notebook solution for Semantic Web (SW) programming that combines the strengths of SPARQL editors with the benefits of notebooks. SWISH gives an integrated notebook experience for the Semantic Web programmer.
demo_108,108,"Mihael Arcan, Mauro Dragoni and Paul Buitelaar",Arcan,The ESSOT System Goes Wild: an Easy Way For Translating Ontologies,demo,Poster and Demo session,12,Mihael Arcan,demo_108.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"To enable knowledge access across languages, ontologies that are often represented only in English, need to be translated into different languages.
Since manual multilingual enhancement of domain-specific ontologies is very time consuming and expensive, smart solutions are required to facilitate the translation task for the language and domain experts.
For this reason, we present ESSOT, an Expert Supporting System for Ontology Translation, which support experts in accomplishing the multilingual ontology management task. Differently than the classic document translation, ontology label translation faces highly specific vocabulary and lack contextual information. 
Therefore, ESSOT takes advantage of the semantic information of the ontology for translation improvement of the ontology labels."
research_68,68,"Giuseppe De Giacomo, Xavier Oriol, Riccardo Rosati and Domenico Fabio Savo",Giacomo,Updating DL-Lite Ontologies through First-Order Queries,research,Reasoning,2,Giuseppe De Giacomo,research_68.pdf,504+505,2016-10-21T15:50:00,2016-10-21T16:10:00,"In this paper we study instance-level update in DL-LiteA, the description logic underlying the OWL 2 QL standard. In particular we focus on formula based approaches to ABox insertion and deletion. We show that DL-LiteA, which is well known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for updates. That is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive DATALOG program. Such a program is readily translatable into a first-order query over the ABox considered as a database, and hence into SQL. Exploiting this result we implement an update component for DL-LiteA-based systems and perform some experiments showing that the approach works in practice."
demo_105,105,"Shuya Abe, Yutaka Mitsuishi, Shinichiro Tago, Nobuyuki Igata, Seiji Okajima, Hiroaki Morikawa and Fumihito Nishino",Abe,Linked Corporations Data in Japan,demo,Poster and Demo session,13,Shuya Abe,demo_105.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Based on Open Data Charter of G8, the governments are publishing corporation register data as Open Data. In Japan, the government recently published a dataset covering approximately 4.4 million corporations, but the dataset is rated as 3 star in the 5-star rating system. Our policy, which we believe is also common in the LOD community, is that low-star datasets must be converted into 5 star as early as possible for strengthening the power of LOD. Based on this policy, we designed a schema for corporation data, converted the Japanese dataset into 5 star using this schema, and published this dataset under Creative Commons Attribution 4.0 License on 9th December 2015, only eight days after the publication date of the original dataset. As far as we know, eight datasets currently refer to ours, which makes the degree of 5 star stronger. As a business purpose, we internally appended links between our dataset and other data such as DBpedia, and applied this enriched data to a visualization system for browsing a corporation from various perspectives."
demo_102,102,"Tabea Tietz, Jörg Waitelonis, Joscha Jäger and Harald Sack",Tietz,refer: a Linked Data based Text Annotation and Recommender System for Wordpress,demo,Poster and Demo session,14,Tabea Tietz,demo_102.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"When searching for an arbitrary subject in weblogs or archives, users often don’t get the information they are really looking for. Often they are overwhelmed with an overflow of information while sometimes the presented information is too scarce to make any use of it. Without further knowledge about the context or background of the intended subject users are easily frustrated because they either cannot handle the amount of information or they might give up because they cannot make sense of the topic at all. Furthermore, authors of online-platforms often deal with the issue to provide useful recommendations of other articles and to motivate the readers to stay on the platform to explore more of the available but most times hidden content of their blog or archive.
In the demo presentation, we present refer, a semantic annotation and visualization system integrated into the Wordpress platform. With refer, content creators are enabled to (semi-)automatically annotate their texts with DBpedia resources as part of the original writing process and visualize them automatically. With refer users are encouraged to take an active part in discovering a platform’s information content interactively and intuitively, rather than just to have to read the entire textual information provided by the author. They can discover background information as well as relationships among persons, places, events, and anything related to the subject in current focus and are inspired to navigate the previously hidden information on a platform.
"
application_15,15,"Vanessa Lopez, Pierpaolo Tommasi, Spyros Kotoulas and Jiewen Wu",Lopez,QuerioDALI: Question Answering over Dynamic and Linked Knowledge Graphs,application,Medical Applications,3,Vanessa Lopez,application_15.pdf,501,2016-10-21T14:10:00,2016-10-21T14:30:00,"We present a domain-agnostic system for Question Answering over multiple semi-structured and possibly linked datasets without the need of a training corpus. The system is motivated by an industry use-case where Enterprise Data needs to be combined with a large body of Open Data to fulfill information needs not satisfied by prescribed application data models. Our proposed Question Answering pipeline combines existing components with novel methods to perform, in turn, linguistic analysis of a query, named entity extraction, entity / graph search, fusion and ranking of possible answers. We evaluate QuerioDALI with two open-domain benchmarks and a biomedical one over Linked Open Data sources, and show that our system produces comparable results to systems that require training data and are domain-dependent. In addition, we analyze the current challenges and shortcomings."
application_13,13,"Evgeny Kharlamov, Bernardo Cuenca Grau, Ernesto Jimenez-Ruiz, Steffen Lamparter, Gulnar Mehdi, Martin Ringsquandl, Yavor Nenov, Sebastian Brandt and Ian Horrocks",Kharlamov,Capturing Industrial Information Models with Ontologies and Constraints,application,Ontologies (I),2,Evgeny Kharlamov,application_13.pdf,501,2016-10-20T15:50:00,2016-10-20T16:10:00,"This paper describes the outcomes of an ongoing collaboration between Siemens and the University of Oxford, with the goal of facilitating the design of ontologies and their deployment in applications. Ontologies are mainly used in Siemens to capture the conceptual information models underpinning a wide range of applications. We start by describing the key role that such models play in two use cases in the manufacturing and energy production sectors. Then, we discuss the formalisation of information models using ontologies, and the relevant reasoning services. Finally, we present SOMM---a tool that supports engineers with little background on semantic technologies in the creation of ontology-based models and in populating them with data. SOMM implements a fragment of OWL 2 RL extended with a form of integrity constraints for data validation, and it comes with support for schema and data reasoning, as well as for model integration. Our evaluation demonstrates the adequacy of SOMM's functionality and performance for Siemens applications."
application_10,10,"Robert Piro, Ian Horrocks, Peter Hendler, Yavor Nenov, Boris Motik, Michael Rossman and Scott Kimberly",Piro,Semantic Technologies for Data Analysis in Health Care,application,Medical Applications,4,Robert Piro,application_10.pdf,501,2016-10-21T14:30:00,2016-10-21T14:50:00,"One focus of Semantic Technologies are formalisms that allow to express complex properties of and relationships between classes of data. The declarative nature of these formalisms is close to natural language and human conceptualisation and thus Semantic Technologies enjoy increasing popularity in scenarios where traditional solutions lead to very convoluted procedures which are difficult to maintain and whose correctness is difficult to judge.
A fruitful application of Semantic Technologies in the field of health care data analysis has emerged from the collaboration between Oxford and Kaiser Permanente a US health care provider (HMO). US HMOs have to annually deliver measurement results on their quality of care to US authorities. One of these sets of measurements is defined in a specification called HEDIS which is infamous amongst data analysts for its complexity. Traditional solutions with either SAS-programs or SQL-queries lead to involved solutions whose maintenance and validation is difficult and binds considerable amount of resources.
In this paper we present the project in which we have applied Semantic Technologies to compute the most difficult part of the HEDIS measures. We show that we arrive at a clean, structured and legible encoding of HEDIS in the rule language of the RDF-triple store RDFox. We use RDFox's reasoning capabilities and SPARQL queries to compute and extract the results. The results of a whole Kaiser Permanente regional branch could be computed in competitive time by RDFox on readily available commodity hardware. Further development and deployment of the project results are envisaged in Kaiser Permanente."
application_18,18,"Evgeny Kharlamov, Yannis Kotidis, Theofilos Mailis, Christian Neuenstadt, Charalampos Nikolaou, Özgür Lütfü Özcep, Christoforos Svingos, Dmitriy Zheleznyakov, Steffen Lamparter, Ian Horrocks, Yannis Ioannidis and Ralf Möller",Kharlamov,Towards Analytics Aware Ontology Based Access to Static and Streaming Data,application,Ontologies (I),3,Evgeny Kharlamov,application_18.pdf,501,2016-10-20T16:10:00,2016-10-20T16:30:00,"Real-time analytics that requires integration and aggregation of heterogeneous and distributed streaming and static data is a typical task in many industrial scenarios such as diagnostics of turbines in Siemens. OBDA approach has a great potential to facilitate such tasks; however, it has a number of limitations in dealing with analytics that restrict its use in important industrial applications. Based on our experience with Siemens, we argue that in order to overcome those limitations OBDA should be extended and become analytics, source, and cost aware. In this work we propose such an extension. In particular, we propose an ontology, mapping, and query language for OBDA, where aggregate and other analytical functions are first class citizens. Moreover, we develop query optimisation techniques that allow to efficiently process analytical tasks over static and streaming data. We implement our approach in a system and evaluate our system with Siemens turbine data."
poster_18,18,Hong Fang and Xiaowang Zhang,Fang,pSPARQL: A Querying Language for Probabilistic RDF (Extended Abstract),poster,Poster and Demo session,16,Hong Fang,poster_18.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper, we present a querying language for probabilistic RDF databases, where each triple has a probability, called pSRARQL, built on SPARQL, recommended by W3C as a querying language for RDF databases. Firstly, we present the syntax and semantics of pSPARQL. Secondly, we define the query problem of pSPARQL corresponding to probabilities of solutions. Finally, we show that the query evaluation of general pSPARQL patterns is PSPACE-complete."
poster_19,19,Monika Solanki,Solanki,Enabling combined software and data engineering: the ALIGNED suite of ontologies,poster,Poster and Demo session,17,Monika Solanki,poster_19.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00," Effective, collaborative integration of software and big data
 engineering for Web-scale systems, is now a crucial technical and
 economic challenge. This requires new combined data and software
 engineering processes and tools. Semantic metadata standards and
 linked data principles, provide a technical grounding for such
 integrated systems given an appropriate model of the domain. In this
 paper we introduce the ALIGNED suite of ontologies specifically
 designed to model the information exchange needs of combined
 software and data engineering. The models have been deployed to
 enable: tool-chain integration, such as the exchange of data quality
 reports; cross-domain communication, such as interlinked data and
 software unit testing; mediation of the system design process
 through the capture of design intents and as a source of context for
 model-driven software engineering processes. These ontologies are
 deployed in web-scale, data-intensive, system development
 environments in both the commercial and academic domains. We
 exemplify the usage of the suite on a complex collaborative software
 and data engineering scenario from the legal information system
 domain."
resource_13,13,"Laura M. Daniele, Monika Solanki, Frank Den Hartog and Jasper Roes",Daniele,Interoperability for Smart Appliances in the IoT World,resource,Smart Planet,4,Laura M. Daniele,resource_13.pdf,504+505,2016-10-19T15:00:00,2016-10-19T15:20:00,"Household appliances are set to become highly intelligent, smart and networked devices in the near future. Systematically deployed on the Internet of Things (IoT), they would be able to form complete energy consuming, producing, and managing ecosystems. Smart systems are technically very heterogeneous, and standardized interfaces on a sensor and device level are therefore needed. However, standardization in IoT has largely focused at the technical communication level, leading to a large number of different solutions based on various standards and protocols, with limited attention to the common semantics contained in the message data structures exchanged at the technical level. The Smart Appliance REFerence ontology (SAREF) is a shared model of consensus developed in close interaction with the industry and with the support of the European Commission. It is published as a technical specification by ETSI and provides an important contribution to achieve semantic interoperability for smart appliances. This paper builds on the success achieved in standardizing SAREF and presents SAREF4EE, an extension of SAREF. SAREF4EE has been created in collaboration with the EEBus and Energy@Home industry associations to interconnect their (different) data models. By using SAREF4EE, smart appliances from different manufacturers that support the EEBus or Energy@Home standards can easily communicate with each other using any energy management system at home or in the cloud."
poster_49,49,"Sejin Chun, Jooik Jung, Xiongnan Jin, Seungjun Yoon and Kyong-Ho Lee",Chun,Proactive Replication of Dynamic Linked Data for Scalable RDF Stream Processing,poster,Poster and Demo session,48,Sejin Chun,poster_49.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper, we propose a proactive replication of Linked Data for RDF Stream Processing. Our solution achieves a fast query processing by replicating subsets of remote RDF datasets before query evaluation. To construct the replication process effectively, we present an update estimation model to handle the changes in updates over time. With the update estimation model, we re-compose instances of the replication process in response to some problems, i.e., the outdated data.
Finally, we conduct exhaustive tests with a real-world dataset to verify
our solution."
poster_10,10,"Terue Takatsuki, Mikako Saito, Sadahiro Kumagai, Eiki Takayama, Kazuya Ohshima, Nozomu Ohshiro, Kai Lenz, Nobuhiko Tanaka, Norio Kobayashi and Hiroshi Masuya",Takatsuki,A RDF based Portal of Biological Phenotype Data produced in Japan,poster,Poster and Demo session,18,Terue Takatsuki,poster_10.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We developed RDF-based databases of phenotype and animal strains produced in Japan and a portal site termed as “J-Phenome”. By the application of common schema, these databases can be retrieved by the same SPARQL query across graphs. In the operation of these databases, RDF represented multiple advantages such as improvement of comprehensive search, data integration using ontologies and public data, reuse of data and wider dissemination of phenotype data compared to conventional technologies."
poster_11,11,"Masao Watanabe, Kazunari Hashimoto, Seiya Inagi, Yohei Yamane, Seiji Suzuki and Hiroshi Umemoto",Watanabe,Working process quantification in factory using wearable sensor device and ontology-based stream data processing,poster,Poster and Demo session,19,Masao Watanabe,poster_11.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"A method for quantifying working processes on manufacturing floors was established that uses a wearable sensor device and an ontology-based stream data processing system.
Using this method, the measurement of manufacturing process efficiency from sensor data extracted from such a device worn by workers on the job was confirmed at the Fuji Xerox factory."
resource_18,18,"Ivan Ermilov, Jens Lehmann, Michael Martin and Sören Auer",Ermilov,LODStats: The Data Web Census Dataset,resource,Linked Data Measurement,3,Ivan Ermilov,resource_18.pdf,504+505,2016-10-20T14:10:00,2016-10-20T14:30:00,"Over the past years, the size of the Data Web has increased significantly, which makes obtaining general insights into its growth and structure both more challenging and more desirable. The lack of such insights hinders important data management tasks such as quality, privacy and coverage analysis. In this paper, we present LODStats, which provides a comprehensive picture of the current state of a significant part of the Data Web. LODStats integrates RDF datasets from data.gov, publicdata.eu and datahub.io data catalogs and at the time of writing lists over 9 000 RDF datasets. For each RDF dataset, LODStats collects comprehensive statistics and makes these available in adhering to the LDSO vocabulary. This analysis has been regularly published and enhanced over the past four years at the public platform lodstats.aksw.org. We give a comprehensive overview over the resulting dataset."
poster_14,14,"Christophe Debruyne, Eamonn Clinton, Lorraine McNerney, Atul Nautiyal and Declan O'Sullivan",Debruyne,Serving Ireland's Geospatial Information as Linked Data,poster,Poster and Demo session,21,Christophe Debruyne,poster_14.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper we present data.geohive.ie, which aims to serve Ireland’s national geospatial data as authoritative Linked Data. Currently, the platform provides information on Irish administrative boundaries and the platform was designed to support two use cases: serving boundary data of geographic features at various level of detail and capturing the evolution of administrative boundaries. We report on the decisions taken for modeling and serving the information such as the adoption of an appropriate URI strategy, the devel-opment of necessary ontologies, and the use of (named) graphs to support the aforementioned use cases. "
research_177,177,"Freddy Brasileiro, Joao Paulo Almeida, Victorio Albani Carvalho and Giancarlo Guizzardi",Brasileiro,Expressive Multi-Level Modeling for the Semantic Web,research,Knowledge Representation,4,Freddy Brasileiro,research_177.pdf,502,2016-10-19T12:00:00,2016-10-19T12:20:00,"In several subject domains, classes themselves may be subject to categorization, resulting in classes of classes (or “metaclasses”). When representing these do-mains, one needs to capture not only entities of different classification levels, but also their (intricate) relations. We observe that this is challenging in current Se-mantic Web languages as there is little support to guide the modeler in producing correct multi-level ontologies, especially because of the nuances in the constraints that apply to entities of different classification levels and their relations. In order to address these representation challenges, we propose a vocabulary that can be used as a basis for multi-level ontologies in OWL along with a number of integri-ty constraints to prevent the construction of inconsistent models. In this process we employ an axiomatic theory called MLT (a Multi-Level Modeling Theory)."
poster_40,40,"Junzhao Zhang, Xiaowang Zhang and Zhiyong Feng",Zhang,IRSMG: Accelerating Inexact RDF Subgraph Matching on the GPU,poster,Poster and Demo session,46,Junzhao Zhang,poster_40.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Many existing approaches have been proposed to solve subgraph matching problem based on filter-and-refine strategy. The efficiency of those existing serial approaches relies on the computational capabilities of CPU. In this paper, we propose an RDF subgraph matching algorithm based on type-isomorphism using GPU since GPU has higher computational performance, more scalability, and lower price than CPU. Firstly, we present a concurrent matching model for type-isomorphism so that subgraph matching can be tackled in a parallel way. Secondly, we develop a parallel algorithm for capturing our proposed concurrent matching model and implement a prototype called IRSMG using GPU. Finally, we evaluate IRSMG on the benchmark datasets LUBM. The experiments show that IRSMG significantly outperforms the state-of-the-art algorithms on the CPU."
research_270,270,"Linhong Zhu, Majid Ghasemi-Gol, Pedro Szekely, Aram Galstyan and Craig Knoblock",Zhu,Unsupervised Entity Resolution on Multi-type Graphs,research,Ontology Matching,4,Linhong Zhu,research_270.pdf,502,2016-10-20T11:30:00,2016-10-20T11:50:00,"We address the problem of performing entity resolution on RDF graphs containing multiple types of nodes, using the links between instances of different types to improve the accuracy. For example, in a graph of products and manufacturers the goal is to resolve all the products and all the manufacturers. We formulate this problem as multi-type graph summarization problem, which involves clustering the nodes in each type that refer to the same entity into one super node and creating weighted links among super nodes that summarize the inter-cluster links in the original graph. Experiments show that the proposed approach outperforms several state-of-the-art generic entity resolution approaches, especially in data sets with one-to-many, many-to-many relations and attributes with missing values."
demo_59,59,"Freddy Lecue, John Vard and Jiewen Wu",Lecue,Using Semantic Web Technologies for Explaining and Predicting Abnormal Expense,demo,Poster and Demo session,32,Freddy Lecue,demo_59.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Travel expenses represent up to 7% of organizations overall budget. Existing expenses systems are designed for reporting expenses types and amount, but not for understanding how to save and spend. We present a system, manipulating semantic web technologies, which aims at identifying, explaining, predicting abnormal expense claims by employees of large organizations in 500+ cities."
demo_112,112,"Michel Buffa, Catherine Faron Zucker, Thierry Bergeron and Hatim Aouzal",Buffa,"Semantic Web Technologies for improving remote visits of museums, using a mobile robot",demo,Poster and Demo session,15,Michel Buffa,demo_112.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The Azkar research project focuses on the remote control of a mobile robot using the emerging Web technologies WebRTC for real time communication. One of the use cases addressed is a remote visit of the French Museum of the Great War in Meaux. For this purpose, we designed an ontology for describing the main scenes in the museum, the objects that compose them, the different trails the robot can follow in a given time period, for a targeted audience, the way points, observation points. This RDF dataset is exploited to assist the human guide in designing a trail, and possibly adapting it during the visit. In this paper we present the Azkar Museum Ontology, the RDF dataset describing some emblematic scenes of the museum, and an experiment that took place in June 2016 with a robot controlled by an operator located 800~kms from the museum. We propose to demonstrate this work in real time during the conference by organizing a remote visit from the conference demo location."
research_78,78,"Imen Megdiche, Olivier Teste and Cassia Trojahn",Megdiche,An Extensible Linear Approach For Holistic Ontology Matching,research,Ontology Matching,2,Imen Megdiche,research_78.pdf,502,2016-10-20T10:50:00,2016-10-20T11:10:00,"Resolving the semantic heterogeneity in the semantic web requires finding correspondences between ontologies describing resources. In particular, with the explosive growth of data sets in the Linked Open Data, linking multiple vocabularies and ontologies simultaneously, known as holistic matching problem, become necessary. Currently, most state-of-the-art matching approaches are limited to pairwise matching. In this paper, we propose an approach for holistic ontology matching that is modeled through a linear program extending the maximum-weighted graph matching problem with linear constraints (cardinality, structural, and coherence constraints). Our approach guarantees the optimal solution with mostly coherent alignments. To evaluate our proposal, we discuss the results of experiments performed on the Conference track of the OAEI 2015, under both holistic and pairwise matching settings."
poster_17,17,"Zhenyu Song, Xiaowang Zhang and Zhiyong Feng",Song,PRONA: A Plugin for Well-Designed Approximate Queries in Jena,poster,Poster and Demo session,20,Zhenyu Song,poster_17.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The time of answering a SPARQL query with its all exact solutions in large scale RDF dataset possibly exceeds users' tolerable waiting time, especially when it contains the OPT operations since the OPT operation is the least conventional operator in SPARQL.
It becomes essential to make a trade-off between the query response time and solution accuracy. We propose PRONA - an plugin for well-designed approximate queries in Jena, which provides help for users to answer well-designed SPARQL queries by approximate computation.The main features of PRONA comprise SPARQL query engine with approximate queries, as well as various approximate degrees for users to choose."
doctoralconsortium_24,24,Syed Muhammad Ali Hasnain,Hasnain,A - Posteriori Data Integration for Life Sciences,doctoralconsortium,Session 1,3,Syed Muhammad Ali Hasnain,doctoralconsortium_24.pdf,,2016-10-18T11:30:00,2016-10-18T11:45:00,"Multiple datasets that add high value to biomedical research have been exposed on the web as part of the Life Sciences Linked Open Data (LS-LOD) Cloud. The ability to easily navigate through these datasets is crucial in order to draw meaningful biological co relations. However, navigating these multiple datasets is not trivial as most of these are only available as isolated SPARQL endpoints with very little vocabulary reuse. We propose an approach for Autonomous Resource Discovery and Indexing (ARDI), a set of configurable rules which can be used to discover links between biological entities in the LS-LOD cloud. We have catalogued and linked concepts and properties from 137 public SPARQL endpoints. The ARDI is used to dynamically assemble queries retrieving data from multiple SPARQL endpoints simultaneously."
poster_15,15,"Makoto Urakawa, Masaru Miyazaki, Hiroshi Fujisawa, Masahide Naemura and Ichiro Yamada",Urakawa,Constructing Curriculum Ontology and Dynamic Learning Path Based on Resource Description Framework,poster,Poster and Demo session,22,Makoto Urakawa,poster_15.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Curriculum for school is generated based on the academic year. For the reason that students need to learn many subjects every year, the relative topics are put into curricula in discrete. In this study, we propose a method to construct a dynamic learning path which enables us to learn the relative topics continuously. In this process, we define two kinds of similarity score, inheritance score and context similarity score to connect the learning path of relative topics. We also construct curriculum ontology with Resource Description Framework (RDF) to make the dynamic learning path accessible. Using the curriculum ontology, we develop a learning system for school which shows a dynamic learning path with broadcasted video clips."
doctoralconsortium_21,21,Takeshi Masuda,Masuda,Ontology Refinement and Evaluation System based on Similarity of Is-a Hierarchies,doctoralconsortium,Session 3,1,Takeshi Masuda,doctoralconsortium_21.pdf,,2016-10-18T16:00:00,2016-10-18T16:15:00,"Ontologies are constructed in various fields such as medical information, mechanical design, and etc. It is important to build high quality ontologies so that these ontologies are used as knowledge bases and knowledge models for application systems. However it is hard to build good quality ontologies because of the necessity of both knowledge of ontology and expertise in their target domain. For this background, ontology construction and refinement costs a lot of time and ef-fort. In order to reduce such costs, we develop an ontology refinement support system. This system have two main function. First, the system can detect points that should be refined and propose how to refine it. Second, the system can evaluate ontologies quantitatively. This system indicate how ontologies are consistent in a classificatory criterion. To develop the refinement support system, we focus on a guideline for building well-organized ontologies that “Each subclass of a super class is distinguished by the values of exactly one attribute of the super class”. When an ontology is built following this guideline, there is similarity among Is-a hierarchies. We use these similar Is-a hierarchies and develop an ontology refinement system."
doctoralconsortium_20,20,Xiangnan Ren,Ren,"Towards a distributed, scalable and real-time RDF Stream Processing engine",doctoralconsortium,Session 2,4,Xiangnan Ren,doctoralconsortium_20.pdf,,2016-10-18T14:45:00,2016-10-18T15:00:00,"Due to the growing need to timely process and derive valuable information and knowledge from data produced in the Semantic Web, RDF stream processing (RSP) has emerged as an important research domain. Of course, modern RSP have to address the volume and velocity characteristics encountered in the Big Data era. This comes at the price of designing high throughput, low latency, fault tolerant, highly available and scalable engines. The cost of implementing such systems from scratch is very high and usually one prefers to program components on top of a framework that possesses these properties, e.g., Apache Hadoop or Apache Spark. The research conducting in this PhD adopts this approach and aims to create a production-ready RSP engine which will be based on domain standards, e.g., Apache Kafka and Spark Streaming. In a nutshell, the engine aims to i) address basic event modeling - to guarantee the completeness of input data in window operators, ii) process real-time RDF stream in a distributed manner - efficient RDF stream handling is required; iii) support and extend common continuous SPARQL syntax - easy-to-use, adapt to the industrial needs and iv) support reasoning services at both the data preparation and query processing levels."
doctoralconsortium_23,23,Sebastian Neumaier,Neumaier,Improving Open Data Usability through Semantics,doctoralconsortium,Session 3,3,Sebastian Neumaier,doctoralconsortium_23.pdf,,2016-10-18T16:30:00,2016-10-18T16:45:00,"With the success of Open Data a huge amount of tabular data become available that could potentially be mapped and linked into the Web of (Linked) Data. The use of semantic web technologies would then allow to explore related content and enhanced search functionalities across data portals. However, existing linkage and labeling approaches mainly rely on mappings of textual information to classes or properties in knowledge bases. In this work we outline methods to recover the semantics of tabular Open Data and to identify related content which allows a mapping and automated integration/categorization of Open Data resources and improves the overall usability and quality of Open Data."
demo_83,83,"Md. Kamruzzaman Sarker, Adila A. Krisnadhi and Pascal Hitzler",Sarker,OWLAx: A Protege Plugin to Support Ontology Axiomatization through Diagramming,demo,Poster and Demo session,41,Md. Kamruzzaman Sarker,demo_83.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Once the conceptual overview, in terms of a somewhat informal class diagram, has been designed in the course of engineering an ontology, the process of adding many of the appropriate logical axioms is mostly a routine task. We provide a Protege plugin which supports this task, together with a visual user interface, based on established methods for ontology design pattern modeling."
poster_28,28,Paolo Pareti,Pareti,Human-Machine Collaboration over Linked Data,poster,Poster and Demo session,24,Paolo Pareti,poster_28.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,This study presents a framework to allow human and machine agents to reason and coordinate actions without direct communication mechanisms by sharing distributed Linked Data resources. This framework addresses the problems of querying frequently-updating distributed datasets and guaranteeing transactional consistency. The motivation for this framework comes from the use-case of opportunistic automation of humans-generated procedures. This use-case is based on existing real-world Linked Data representations of human instructions and their integration with machine functionalities.
research_163,163,"Victor Christen, Anika Groß and Erhard Rahm",Christen,A Reuse-based Annotation Approach for Medical Documents,research,Medical Applications,2,Victor Christen,research_163.pdf,501,2016-10-21T13:50:00,2016-10-21T14:10:00,"Annotations are useful to semantically enrich documents and other datasets with concepts of standardized vocabularies and ontologies. In the medical domain, many documents are not annotated at all and manual annotation is a difficult and time-consuming process. Therefore, automatic annotation methods become necessary to support human annotators with recommendations. We propose a reuse-based annotation approach that clusters items in medical documents according to verified ontology-based annotations. We identify a set of representative features for annotation clusters and propose a context-based selection strategy that considers the semantic relatedness and frequent co-occurrences of annotated concepts. We evaluate our methods and the annotation tool MetaMap based on reference mappings between medical forms and the Unified Medical Language System."
research_167,167,"Ilaria Tiddi, Mathieu d'Aquin and Enrico Motta",Tiddi,Learning to Assess Linked Data Relationships Using Genetic Programming,research,Linked Data Measurement,2,Ilaria Tiddi,research_167.pdf,504+505,2016-10-20T13:50:00,2016-10-20T14:10:00,"The goal of this work is to learn a measure supporting the detection of strong relationships between Linked Data entities. Such relationships can be represented as paths of entities and properties, and can be obtained through a blind graph search process traversing Linked Data. The challenge here is therefore the design of a cost-function that is able to detect the strongest relationship between two given entities, by objectively assessing the value of a given path. To achieve this, we use a Genetic Programming approach in a supervised learning method to generate path evaluation functions that compare well with human evaluations. We show how such a cost-function can be generated only using basic topological features of the nodes of the paths as they are being traversed (i.e. without knowledge of the whole graph), and how it can be improved through introducing a very small amount of knowledge about the vocabularies of the properties that connect nodes in the graph."
poster_21,21,Olaf Hartig and Carlos Buil Aranda,Hartig,Reducing the Network Load of Triple Pattern Fragments by Supporting Bind Joins,poster,Poster and Demo session,26,Olaf Hartig,poster_21.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The recently proposed Triple Pattern Fragment (TPF) interface aims at increasing the availability of Web-queryable RDF datasets by trading off an increased client-side query processing effort for a significant reduction of server load. However, an additional aspect of this trade-off is a very high network load. To mitigate this drawback we propose to extend the interface by allowing clients to augment TPF requests with a VALUES clause as introduced in SPARQL 1.1. In an ongoing research project we study the trade-offs of such an extended TPF interface and compare it to the pure TPF interface. With a poster in the conference we aim to present initial results of this research. In particular, we would like to present a series of experiments showing that a distributed, bind-join-based query execution using this extended interface can reduce the network load drastically (in terms of both the number of HTTP requests and data transfer)."
research_164,164,Olaf Hartig and M. Tamer Ozsu,Hartig,Walking without a Map: Ranking-Based Traversal for Querying Linked Data,research,Search (II),4,Olaf Hartig,research_164.pdf,504+505,2016-10-20T16:30:00,2016-10-20T16:50:00,"The emergence of Linked Data on the WWW has spawned research interest in an online execution of declarative queries over this data. A particularly interesting approach is traversal-based query execution which fetches data by traversing data links and, thus, is able to make use of up-to-date data from initially unknown data sources. While the downside of this approach is the delay before the query engine completes a query execution, user perceived response time may be improved significantly by returning as many elements of the result set as soon as possible. To this end, the query engine requires a traversal strategy that enables the engine to fetch result-relevant data as early as possible. The challenge for such a strategy is that the query engine does not know a priori what data sources will be discovered during the query execution and which of them contain result-relevant data. In this paper, we investigate 14 different approaches to rank traversal steps and achieve a variety of traversal strategies. We experimentally study their impact on response times and compare them to a baseline that resembles a breadth-first traversal. While our experiments show that some of the approaches can achieve noteworthy improvements over the baseline in a significant number of cases, we also observe that for every approach, there is a non-negligible chance to achieve response times that are worse than the baseline."
research_242,242,"Bahaa Eldesouky, Menna Bakry, Heiko Maus and Andreas Dengel",Eldesouky,"Seed, an End-user Text Composition Tool for the Semantic Web",research,Interaction,1,Bahaa Eldesouky,research_242.pdf,504+505,2016-10-19T11:00:00,2016-10-19T11:20:00,"Despite developments of Semantic Web-enabling technologies,
the gap between non-expert end-users and the Semantic Web still
exists. In the field of semantic content authoring, tools for interacting
with semantic content remain directed at highly trained individuals. This
adds to the challenges of bringing user-generated content into the Semantic
Web.
In this paper, we present Seed, short for Semantic Editor, an extensible
knowledge-supported natural language text composition tool, which
targets non-experienced end-users enabling automatic as well as semiautomatic
creation of standards based semantically annotated textual
content. We point out the structure of Seed, compare it with related
work and explain how it utilizes Linked Open Data and state of the art
Natural Language Processing to realize user-friendly generation of textual
content for the Semantic Web. We also present experimental evaluation
results involving a diverse group of more than 120 participants,
which showed that Seed helped end-users easily create and interact with
semantic content with nearly no prerequisite knowledge."
demo_54,54,Jędrzej Potoniec,Potoniec,An On-Line Learning to Query System,demo,Poster and Demo session,28,Jędrzej Potoniec,demo_54.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,We present an on-line system which learns a SPARQL query from a set of wanted and a set of unwanted results of the query. The sets are extended during a dialog with the user. The system leverages SPARQL 1.1 and does not depend on any particular RDF graph.
resource_81,81,"Efstratios Sygkounas, Giuseppe Rizzo and Raphaël Troncy",Sygkounas,A Replication Study of the Top Performing Systems in SemEval Twitter Sentiment Analysis,resource,Natural Language Processing,3,Efstratios Sygkounas,resource_81.pdf,504+505,2016-10-21T11:10:00,2016-10-21T11:30:00,"We performed a thorough replicate study of the top systems performing in the yearly SemEval Twitter Sentiment Analysis task. We highlight some differences between the results obtained by the top systems and the ones we are able to compute. We also propose SentiME, an ensemble system composed of 5 state-of-the-art sentiment classifiers. SentiME first trains the different classifiers using the Bootstrap Aggregating Algorithm. The classification results are then aggregated using a linear function that averages the classification distributions of the different classifiers. SentiME has also been tested over the SemEval2015 test set, properly trained with the SemEval2015 train test, outperforming the best ranked system of the challenge.

"
demo_52,52,"Michel Héon, Roger Nkambou and Mohamed Gaha",Héon,OntoCASE4G-OWL: Towards a modeling software tool for G-OWL a visual syntax for RDF/RDFS/OWL2,demo,Poster and Demo session,29,Michel Héon,demo_52.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Ontological syntax standardized by the W3C offer the expressiveness needed in the formulation of complex concepts. However, the codification of an ontology is a process of formalization of thought that sometimes requires extensive knowledge and is often inaccessible in the layperson's logic. The G-OWL (for Graphical OWL) language has been designed to provide a tool to facilitate the expression of knowledge in a manner that is compatible with the OWL2 ontolo-gy. This paper presents the OntoCASE4G-OWL prototype, a visual modeling software for the editing of formal ontologies in G-OWL and their translation into Turtle. The executable version of OntoCASE for Windows and MacOsX is available at http://www.cotechnoe.com/iswc2016"
resource_87,87,"Alberto Tonon, Djellel Eddine Difallah, Victor Felder and Philippe Cudré-Mauroux",Tonon,VoldemortKG: Mapping Schema.org Entities to Linked Open Data,resource,Knowledge Graph,1,Alberto Tonon,resource_87.pdf,501,2016-10-19T11:00:00,2016-10-19T11:20:00,"Increasingly, Web pages mix entities coming from different sources and represented in several different ways. It can thus happen that the same entity is both described by using schema.org annotations and by creating a text anchor pointing to its Wikipedia page. Often, those representations provide complementary information which is not exploited since those entities are disjoint.

In this project, we explore the extent to which entities represented in different ways repeat on the Web, how they are related, and how they complement (or link) to each other. Our initial experiments show that we can unveil a previously (unexploited) knowledge graph by applying simple instance matching techniques on a large collection of schema.org annotations and DBpedia. The resulting knowledge graph aggregates entities (often tail entities) scattered across several Web pages, and complements existing DBpedia entities with new facts and properties.

In order to facilitate further investigations in how to mine such information, we are releasing i) an excerpt of all CommonCrawl web pages containing both Wikipedia and schema.org annotations, ii) the toolset to extract this information and perform knowledge graph construction and mapping onto DBpedia, as well as iii) the resulting knowledge graph (VoldemortKG) obtained via label matching techniques."
resource_88,88,"Vincent Link, Steffen Lohmann and Florian Haag",Link,OntoBench: Generating Custom OWL 2 Benchmark Ontologies,resource,Reasoning,4,Vincent Link,resource_88.pdf,504+505,2016-10-21T16:30:00,2016-10-21T16:50:00,"A variety of tools for visualizing, editing, and documenting OWL ontologies have been developed in the last couple of years. The OWL coverage and conformance of these tools usually needs to be tested during development or for evaluation and comparison purposes. However, in particular for the testing of special OWL concepts and concept combinations, it can be tedious to find suitable ontologies and test cases. We have developed OntoBench, a generator for OWL 2 benchmark ontologies that can be used to test and compare ontology visualizers and related tools. In contrast to existing OWL benchmarks, OntoBench does not focus on scalability and performance but OWL coverage and concept combinations. Consistent benchmark ontologies are dynamically generated based on OWL 2 language constructs selected in a graphical user interface. OntoBench is available on GitHub and as a public service, making it easy to use the tool and generate custom ontologies or ontology fragments."
poster_4,4,"Paramita Mirza, Simon Razniewski and Werner Nutt",Mirza,"Expanding Wikidata's Parenthood Information by 178%, or How To Mine Relation Cardinality Information",poster,Poster and Demo session,30,Paramita Mirza,poster_4.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"While automated knowledge base construction so far has largely focused on fully qualified facts, e.g. <Obama, hasChild, Malia>, the Web contains also extensive amounts of cardinality information, such as that someone has two children without giving their names. In this paper we argue that the extraction of such information could substantially increase the scope of knowledge bases. For the sample of the hasChild relation in Wikidata, we show that simple regular-expression based extraction from Wikipedia can increase the size of the relation by 178. We also show how such cardinality information can be used to estimate the recall of knowledge bases."
demo_58,58,"Stefano Faralli, Christian Bizer, Kai Eckert, Robert Meusel and Simone Paolo Ponzetto",Faralli,A Web Application to Search a Large Repository of Taxonomic Relations from the Web,demo,Poster and Demo session,31,Stefano Faralli,demo_58.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Taxonomic relations (also known as ``isa'' relations or hypernymy relations) represent a fundamental atomic piece of structured information for many text understanding applications. Such structured information is part of the basic topology structure of knowledge bases and foundational ontologies. Despite the availability of shared knowledge bases, some NLP applications (e.g. Ontology Learning) require automatic isa relation harvesting techniques to cope with the coverage of domain-specific and long-tail terms. We present a Web Application to directly query our repository of isa relations extracted from the Common Crawl (the largest publicly available crawl of the Web). Our resource can be also downloaded for research purposes and accessed programmatically (we also release a Java application programming interface). "
poster_7,7,Khai Nguyen and Ryutaro Ichise,Nguyen,Ranking Feature for Classifier-based Instance Matching,poster,Poster and Demo session,23,Khai Nguyen,poster_7.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Instance matching is the problem of finding the instances that describe the same object. It can be viewed as a classification problem, where a pair of two instances is predicted as match or non-match. A common limitation of existing classifier-based matching systems is the absent of instance pairs ranking.
We propose using a ranking feature to enhance the classifier in instance matching. 
Experiments on real datasets confirm the significant improvement when applying our method."
journal_7,7,"John P. McCrae, Mihael Arcan, Kartik Asooja, Jorge Gracia, Paul Buitelaar and Philipp Cimiano",McCrae,Domain Adaptation for Ontology Localization,journal,Multilinguality,4,John P. McCrae,journal_7.pdf,501,2016-10-19T15:00:00,2016-10-19T15:20:00,"Ontology localization is the task of adapting an ontology to a different cultural context, and has been identified as an important task in the context of the Multilingual Semantic Web vision. The key task in ontology localization is translating the lexical layer of an ontology, i.e., its labels, into some foreign language. For this task, we hypothesize that the translation quality can be improved by adapting a machine translation system to the domain of the ontology. To this end, we build on the success of existing statistical machine translation (SMT) approaches, and investigate the impact of different domain adaptation techniques on the task. In particular, we investigate three techniques: (i) enriching a phrase table by domain-specific translation candidates acquired from existing Web resources, (ii) relying on Explicit Semantic Analysis as an additional technique for scoring a certain translation of a given source phrase, as well as (iii) adaptation of the language model by means of weighting n-grams with scores obtained from topic modelling. We present in detail the impact of each of these three techniques on the task of translating ontology labels. We show that these techniques have a generally positive effect on the quality of translation of the ontology and that, in combination, they provide a significant improvement in quality. "
demo_77,77,"Matias Junemann, Juan L. Reutter, Adrian Soto and Domagoj Vrgoc",Junemann,Incorporating API data into SPARQL query answers,demo,Poster and Demo session,88,Matias Junemann,demo_77.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this demo we present an extension of SPARQL which allows queries to connect to JSON APIs and integrate the obtained information into query answers. We achieve this by adding a new operator to SPARQL, and implement this extension on top of the Jena framework in order to illustrate how it functions with real world APIs."
poster_44,44,"Lihua Zhao, Naoya Arakawa, Hiroaki Wagatsuma and Ryutaro Ichise",Zhao,An Ontology based Map Converter for Intelligent Vehicles,poster,Poster and Demo session,44,Lihua Zhao,poster_44.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Sophisticated digital map is an essential resource for intelligent vehicles to localize and retrieve environment information. However, the open map source do not contain enough information for decision making during autonomous driving. Although comprehensive commercial map data can provide precise map knowledge, the data format is not in a machine-readable format. Therefore, we retrieve useful knowledge from high-precision commercial map and convert it into ontology based data to help intelligent vehicles perceive driving environment and make decisions at various traffic scenarios. Other than developing deci- sion making systems, the converted map data can be used as a golden standard for evaluating traffic sign detection, road mark detection, and automatic map construction."
application_33,33,"Mihael Arcan, Mauro Dragoni and Paul Buitelaar",Arcan,Translating Ontologies in a Real-World Setting with ESSOT,application,Multilinguality,2,Mihael Arcan,application_33.pdf,501,2016-10-19T14:20:00,2016-10-19T14:40:00,"To enable knowledge access across languages, ontologies that are often represented only in English, need to be translated into different languages.
The main challenge in translating ontologies is to find the right term with respect to the domain modeled by ontology itself.
Machine translation services may help in this task; however, a crucial requirement is to have translations validated by experts before the ontologies are deployed.
Real-world applications must implement a support system addressing this task for relieve experts work in validating all translations.
In this paper, we present ESSOT, an Expert Supporting System for Ontology Translation.
The peculiarity of this system is to exploit semantic information of the concept's context for improving the quality of label translations.
The system has been tested both within the Organic.Lingua project by translating the modeled ontology in three languages and on other multilingual ontologies in order to evaluate the effectiveness of the system in other contexts.
The results have been compared with the translations provided by the Microsoft Translator API and the improvements demonstrated the viability of the proposed approach."
application_36,36,"Francesco Osborne, Angelo Salatino, Aliaksandr Birukou and Enrico Motta",Osborne,Automatic Classification of Springer Nature Proceedings with Smart Topic Miner,application,Data Mining,3,Francesco Osborne,application_36.pdf,502,2016-10-21T11:10:00,2016-10-21T11:30:00,"The process of classifying scholarly outputs is crucial to ensure timely access to knowledge. However, this process is typically carried out manually by expert editors, leading to high costs and slow throughput. In this paper we present Smart Topic Miner (STM), a novel solution which uses semantic web technologies to classify scholarly publications on the basis of a very large automatically generated ontology of research areas. STM was developed to support the Springer Nature Computer Science editorial team in classifying proceedings in the LNCS family. It analyses in real time a set of publications provided by an editor and produces a structured set of topics and a number of Springer Nature classification tags, which best characterise the given input. In this paper we present the architecture of the system and report on an evaluation study conducted with a team of Springer Nature editors. The results of the evaluation, which showed that STM classifies publications with a high degree of accuracy, are very encouraging and as a result we are currently discussing the required next steps to ensure large scale deployment within the company."
application_38,38,"Kavitha Srinivas, Julian Dolby, Achille Fokoue, Mariano Rodríguez Muro and Wen Sun",Srinivas,Extending SPARQL for data analytic tasks,application,Querying/SPARQL (I),1,Kavitha Srinivas,application_38.pdf,501,2016-10-20T10:30:00,2016-10-20T10:50:00,"SPARQL has many nice features for accessing data integrated across different data sources, which is an important step in any data analysis task. We report the use of SPARQL for two real data analytic use cases from the healthcare and life sciences domains, which exposed certain weaknesses in the current specification of SPARQL; specifically when the data being integrated is most conveniently accessed via RESTful services and in formats beyond RDF, such as XML. We therefore extended SPARQL with generalized 'service', constructs for accessing services beyond the SPARQL endpoints supported by 'service'; for efficiency, our constructs additionally needed to support posting data, which is also not supported by 'service'. Furthermore, data from multiple sources led to natural modularity in the queries, with different portions of the query pertaining to different sources, so we also extended SPARQL with a simple 'function' mechanism to isolate the mechanics of accessing each endpoint. We provide an open source implementation of this SPARQL endpoint in an RDF store called Quetzal, and evaluate its use in the two data analytic scenarios over real datasets."
poster_41,41,Fabien Gandon,Gandon,Materializing the editing history of Wikipedia as linked Data in DBpedia,poster,Poster and Demo session,45,Fabien Gandon,poster_41.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,We describe a DBpedia extractor materializing as linked data the editing history of Wikipedia pages to support historical queries and indicators.
resource_73,73,"Amit Joshi, Pascal Hitzler and Guozhu Dong",Joshi,LinkGen: Multipurpose Linked Data Generator,resource,Linked Data,3,Amit Joshi,resource_73.pdf,504+505,2016-10-20T11:10:00,2016-10-20T11:30:00,"The paper presents a synthetic linked data generator that can generate a large amount of RDF data based on certain statistical distribution. Data generation is platform independent, supports streaming mode and produces output in N-Triples and N-Quad format. Different sets of output can be generated using various configuration parameters and the outputs are reproducible. Unlike existing generators, our generator accepts any vocabulary and can supplement the output with noisy and inconsistent data. The generator has an option to inter-link instances with real ones provided that the user supplies entities from real datasets."
resource_25,25,"Andrea Giovanni Nuzzolese, Anna Lisa Gentile, Valentina Presutti and Aldo Gangemi",Nuzzolese,Conference Linked Data: the ScholarlyData project,resource,Linked Data,1,Andrea Giovanni Nuzzolese,resource_25.pdf,504+505,2016-10-20T10:30:00,2016-10-20T10:50:00,"The Semantic Web Dog Food (SWDF) is the reference linked dataset of the Semantic Web community about papers, people, organisations, and events related to its academic conferences. In this paper we analyse the existing problems of generating, representing and maintaining Linked Data for the SWDF. With this work (i) we provide a refactored and cleaned SWDF dataset; (ii) we use a novel data model which improves the Semantic Web Conference Ontology, adopting best ontology design practices and (iii) we provide an open source maintenance workflow to support a healthy grow of the dataset beyond the Semantic Web conferences."
research_156,156,Petar Ristoski and Heiko Paulheim,Ristoski,RDF2Vec: RDF Graph Embeddings for Data Mining,research,Data Mining,2,Petar Ristoski,research_156.pdf,502,2016-10-21T10:50:00,2016-10-21T11:10:00,"Linked Open Data has been recognized as a valuable source for background information in data mining. However, most data mining tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. Our evaluation shows that such vector representations outperform existing techniques for the propositionalization of RDF graphs on a variety of different predictive machine learning tasks, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks."
research_88,88,Lei Zhang and Achim Rettinger,Zhang,A Probabilistic Model for Time-Aware Entity Recommendation,research,Search (I),4,Lei Zhang,research_88.pdf,502,2016-10-20T14:30:00,2016-10-20T14:50:00,"In recent years, there has been an increasing efforts to develop techniques for related entity recommendation, where the task is to retrieve a ranked list of related entities given a keyword query. Another trend in the area of information retrieval (IR) is to take temporal aspects of a given query into account when assessing the relevance of documents. However, while this has become an established functionality in document search engines, the significance of time, especially when explicitly given, has not been recognized for entity recommendation, yet. We address this gap by introducing the task of time-aware entity recommendation. We propose the first probabilistic model that takes time-awareness into consideration for entity recommendation by leveraging heterogeneous knowledge of entities extracted from different data sources publicly available on the Web. We extensively evaluate the proposed approach and our experimental results show considerable improvements compare to time-agnostic entity recommendation approaches."
poster_43,43,"Xiang Nan Ren, Olivier Curé, Houda Khrouf, Zakia Kazi-Aoul and Yousra Chabchoub",Ren,Apache Spark and Apache Kafka at the rescue of distributed RDF Stream Processing engines,poster,Poster and Demo session,47,Xiang Nan Ren,poster_43.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Due to the growing need to timely process and derive valuable information and knowledge from data produced in the Semantic Web, RDF stream processing (RSP) has emerged as an important research domain. In this paper, we describe the design of an RSP engine that is built upon state of the art Big data frameworks, namely Apache Kafka and Apache Spark. Together, they support the implementation of a production-ready RSP engine that guarantees scalability, fault-tolerance, high availability, low latency and high throughput. Moreover, we highlight that the Spark framework considerably eases the implementation of complex applications requiring libraries as diverse as machine learning, graph processing, query processing and stream processing."
research_85,85,"Anthony Potter, Boris Motik, Yavor Nenov and Ian Horrocks",Potter,Distributed RDF Query Answering with Dynamic Data Exchange,research,Querying/SPARQL (II),1,Anthony Potter,research_85.pdf,502,2016-10-21T13:30:00,2016-10-21T13:50:00,"Evaluating joins over RDF data stored in a shared-nothing server cluster is key
to processing truly large RDF datasets. To the best of our knowledge, the
existing approaches use a variant of the data exchange operator that is
inserted into the query plan statically (i.e., at query compile time) to
shuffle data between servers. We argue that this often misses opportunities for
local computation, and we present a novel solution to distributed query
answering that consists of two main components. First, we present a query
answering algorithm based on dynamic data exchange, which exploits data
locality better than the static approaches. Second, we present a partitioning
algorithm for RDF data based on graph partitioning whose aim is to increase
data locality. We have implemented our approach in the RDFox system, and our
performance evaluation suggests that our techniques outperform the state of the
art by up to an order of magnitude."
research_149,149,"Armen Inants, Manuel Atencia and Jérôme Euzenat",Inants,Algebraic calculi for weighted ontology alignments,research,Ontology Matching,1,Armen Inants,research_149.pdf,502,2016-10-20T10:30:00,2016-10-20T10:50:00,"Alignments between ontologies usually come with numerical attributes expressing the confidence of each correspondence. Semantics supporting such confidences must generalise the semantics of alignments without confidence. There exists a semantics which satisfies this but introduces a discontinuity between weighted and non-weighted interpretations. Moreover, it does not provide a calculus for reasoning with weighted ontology alignments. This paper introduces a calculus for such alignments. It is given by an infinite relation-type algebra, the elements of which are weighted taxonomic relations. In addition, it approximates the non-weighted case in a continuous manner."
demo_20,20,"Eugene Siow, Thanassis Tiropanis and Wendy Hall",Siow,PIOTRe: Personal Internet of Things Repository,demo,Poster and Demo session,34,Eugene Siow,demo_20.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Resource-constrained Internet of Things (IoT) devices like Raspberry Pis', with specific performance optimisation, can serve as interoperable personal Linked Data repositories for IoT applications. In this demo paper we describe PIOTRe, a personal datastore that utilises our sparql2sql query translation technology on Pis' to process, store and publish IoT time-series historical data and streams. We demonstrate, for a smart home scenario with PIOTRe: a real-time dashboard that utilises RDF stream processing, a set of descriptive analytics visualisations on historical data, a framework for registering stream queries within a local network and a means of sharing metadata globally with HyperCat and Web Observatories."
demo_27,27,"Francesco Osborne, Angelo Antonio Salatino, Aliaksandr Birukou and Enrico Motta",Osborne,Smart Topic Miner: Supporting Springer Nature Editors with Semantic Web Technologies,demo,Poster and Demo session,35,Francesco Osborne,demo_27.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Academic publishers, such as Springer Nature, annotate scholarly products with the appropriate research topics and keywords to facilitate the marketing process and to support (digital) libraries and academic search engines. This critical process is usually handled manually by experienced editors, leading to high costs and slow throughput. In this demo paper, we present Smart Topic Miner (STM), a semantic application designed to support the Springer Nature Computer Science editorial team in classifying scholarly publications. STM analyses conference proceedings and annotates them with a set of topics drawn from a large automatically generated ontology of research areas and a set of tags from Springer Nature Classification. "
demo_26,26,"Fabian M. Suchanek, Colette Menard, Meghyn Bienvenu and Cyril Chapellier",Suchanek,What if machines could be creative?,demo,Poster and Demo session,36,Fabian M. Suchanek,demo_26.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this demo proposal, we present a system that proposes generations of existing concepts such as ""cars that park automatically""or ""skyscrapers made of glass""."
doctoralconsortium_8,8,Valentina Ivanova,Ivanova,Applications of Large Displays: Advancing User Support in Large Scale Ontology Alignment,doctoralconsortium,Session 3,2,Valentina Ivanova,doctoralconsortium_8.pdf,,2016-10-18T16:15:00,2016-10-18T16:30:00,"Producing alignments of highest quality requires ‘humans in the loop’, however, user involvement is currently one of the challenges for the ontology alignment community. Ontology alignment is a cognitively intensive task and could be efficiently supported by user interfaces encompassing well-designed visualizations and interaction techniques. This work investigates the application of large, high-resolution displays to improve users’ cognitive support and identifies several promising directions for their application—improving ontologies’ and alignments’ navigation, supporting users’ thinking process and collaboration."
doctoralconsortium_9,9,Muhammad Amith,Amith,Ontology-based dialogue systems for improved patient HPV vaccine knowledge and perception,doctoralconsortium,Session 1,2,Muhammad Amith,doctoralconsortium_9.pdf,,2016-10-18T11:15:00,2016-10-18T11:30:00,"In Sir Tim Berners-Lee’s seminal article that introduce his vision of the semantic web, one of the use-cases described was a health- related example where health consumers utilized intelligent hand-held devices that aggregated and exchanged health data from the semantic web. Presently, majority of health consumers and patients rely on personal technology and the web to find information and to make personal health decisions. This proposal aims to contribute towards that use-case, specifically in the “hot-bed” issue of human papillomavirus (HPV) vac- cine. The HPV vaccine targets young adults and teens to protect against life-threatening cancers, yet a segment of the public has reservations against the vaccine. I propose an interactive dialogue agent that harness patient-level vaccine information encoded in an ontology that can be “talked to” with a natural language interface using utterances. I aim to pilot this technology in a clinic to assess if patient knowledge about HPV and the vaccine is increased, and if their attitude toward the vaccine is modified as a result of using the interactive agent."
doctoralconsortium_4,4,Kleanthi Georgala,Georgala,Scalable Link Discovery for Modern Data-Driven Applications,doctoralconsortium,Session 2,5,Kleanthi Georgala,doctoralconsortium_4.pdf,,2016-10-18T15:00:00,2016-10-18T15:15:00,"Modern data-driven applications often have to integrate and process large volumes of high-velocity data. To this end, they require fast and accurate Link Discovery solutions. Most Link Discovery frameworks rely on complex link specifications to determine candidates for links. Hence, the main focus of this work lies in the conception, development, implementation and evaluation of time-efficient and scalable Link Discovery approaches based on the link specification paradigm. We address the aforementioned challenges by presenting approaches for (1) time-constrained linking and (2) for the efficient computation and (3) scalable execution of link specifications with applications to periodically updated knowledge bases. The overall result of this thesis will be an open-source framework for link discovery on large volumes of RDF data streams."
research_304,304,"Isa Guclu, Yuan-Fang Li, Jeff Z. Pan and Martin J. Kollingbaum",Guclu,Predicting Energy Consumption of Ontology Reasoning over Mobile Devices,research,Smart Planet,1,Isa Guclu,research_304.pdf,504+505,2016-10-19T14:00:00,2016-10-19T14:20:00,"The unprecedented growth in mobile devices, combined with advances in Semantic Web Technologies, has given birth to opportunities for more intelligent systems on-the-go. Limited resources of mobile devices, especially energy, demand approaches to make mobile reasoning more applicable. While Mobile-Cloud integration is a promising method for harnessing the power of semantic technologies in the mobile infrastructure, it is an open question on deciding when to reason with ontologies on mobile devices. In this paper, we introduce an energy consumption prediction mechanism for ontology reasoning on mobile devices, which allows analysis of feasibility of ontology reasoning on mobile devices in terms of energy consumption. The prediction models contributes to mobile-cloud integration and helps improve further development of ontology and semantic solutions in general."
doctoralconsortium_2,2,Le Tuan Anh,Anh,Linked Data processing for Embedded Devices,doctoralconsortium,Session 1,4,Le Tuan Anh,doctoralconsortium_2.pdf,,2016-10-18T11:45:00,2016-10-18T12:00:00,"Our PhD work aims to a comprehensive, scalable and resourced-awareness
software framework to process RDF data for embedded devices.
In this proposal, we introduce a system architecture supporting RDF storage,
SPARQL query, RDF reasoning and continuous query for RDF stream. The ar-
chitecture is designed to be applicable to embedded systems. For the efficient
performance and scalability, we propose data management techniques adapt-
ing to hardware characteristics of embedded devices. Since computing resources
on embedded devices are constraint, their usage should be context dependent.
Therefore, we work on a resource adaptation model that supports trading off
system performance and device resources depending on their availability. The
adaptation model is based on the resource cost model of the data management
techniques."
demo_84,84,"Fernando Florenzano, Denis Parra, Juan L. Reutter and Freddie Venegas",Florenzano,An interactive visualisation for RDF data,demo,Poster and Demo session,39,Fernando Florenzano,demo_84.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We demonstrate a visualisation aimed at facilitating SPARQL-fluent 
users to produce queries over a dataset they are not familiar with. 
This visualisation consists of a labelled graph whose nodes are the different types of entities in the RDF dataset, 
and where two types are related if entities of these types appear related in the RDF dataset. To avoid a visual overload when 
the number of types in a dataset is too big, the graph groups together all types that are subclass of a 
more general type, and users are given the option of navigating through this hierarchy of types, dividing type nodes 
into subtypes as they see fit. 
We illustrate our visualisation using the Linked Movie Database dataset, and offer as well the visualisation of DBpedia. 
"
demo_80,80,"Thi-Nhu Nguyen, Hideaki Takeda, Khai Nguyen, Ryutaro Ichise and Tuan-Dung Cao",Nguyen,Type Prediction for Entities in DBpedia by Aggregating Multilingual Resources,demo,Poster and Demo session,40,Thi-Nhu Nguyen,demo_80.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The entity type is considered as very important in DBpedia. Since this information is inconsistently described in different languages, it is difficult to recognize the most suitable type of an entity. We propose a method to predict the entity type based on a novel conformity measure. We combine the consideration of the specific-level of and the majority voting. The experiment result shows that our method can suggest informative types and outperforms the baselines."
application_29,29,"Tong Ruan, Lijuan Xue, Haofen Wang, Fanghuai Hu, Liang Zhao and Jun Ding",Ruan,Building and Exploring National-wide Enterprise Knowledge Graphs for Investment Analysis in an Incremental Way,application,Knowledge Graph,2,Tong Ruan,application_29.pdf,501,2016-10-19T11:20:00,2016-10-19T11:40:00,"Full-fledged enterprise information can be a great weapon in investment analysis. However, enterprise information is scattered in different databases and websites. The information from a single source is incomplete and also suffers from noise. It is not an easy task to integrate and utilize information from diverse sources in real business scenarios. In this paper, we present an approach to build knowledge graphs (KGs) by exploiting semantic technologies to reconcile the data from diverse sources incrementally. We build a national-wide enterprise KG which incorporates information about 40,000,000 enterprises in China. We also provide querying about enterprises and data visualization capabilities as well as novel investment analysis scenarios, including finding an enterprise's real controllers, innovative enterprise analysis, enterprise path discovery and so on. The KG and its applications are currently used by two security companies in their investment banking businesses."
research_224,224,"Michael Cochez, Stefan Decker and Eric Prud'Hommeaux",Cochez,Knowledge Representation on the Web revisited: the Case for Prototypes,research,Knowledge Representation,1,Michael Cochez,research_224.pdf,502,2016-10-19T11:00:00,2016-10-19T11:20:00,"In recent years RDF and OWL have become the most common knowledge representation languages in use on the Web, propelled by the recommendation of the W3C. In this paper we examine an alternative way to represent knowledge based on Prototypes. This Prototype based representation has different properties, which we argue to be 
more suitable for data sharing and reuse on the Web. Prototypes avoid the distinction between classes and instances and provide means for objects based data sharing and reuse.

In this paper we discuss the requirements and design principles for Knowledge Representation based on Prototypes on the Web, after which we propose a formal syntax and semantics. We show how to embed knowledge representation based on Prototypes in the current Semantic Web standard stack. An implementation and practical evaluation of the system is presented in a separate resource paper."
demo_88,88,Suvodeep Mazumdar and Ziqi Zhang,Mazumdar,Visualizing Semantic Table Annotations with TableMiner+,demo,Poster and Demo session,42,Suvodeep Mazumdar,demo_88.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"This paper describes an extension of the TableMiner+ sys- tem, the only open source Semantic Table Interpretation system that annotates Web tables using Linked Data in an effective and efficient ap- proach. It adds a graphical user interface to TableMiner+, to facilitate the visualization and correction of automatically generated annotations. This makes TableMiner+ an ideal tool for the semi-automatic creation of high-quality semantic annotations on tabular data, which facilitates the publication of Linked Data on the Web."
application_21,21,"Bruno Charron, Yu Hirate, David Purcell and Martin Rezk",Charron,Extracting Semantic Information for e-Commerce,application,Enriching Data Sources,4,Bruno Charron,application_21.pdf,502,2016-10-21T16:30:00,2016-10-21T16:50:00,"Rakuten Ichiba uses a taxonomy to organize the items it sells. Currently, the taxonomy classes that are relevant in terms of profit generation and difficulty of exploration are being manually extended with data properties deemed helpful to create pages that improve the user search experience and ultimately the conversion rate. In this paper we present a scalable approach that aims to automate this process, automatically selecting the relevant and semantically homogenous subtrees in the taxonomy, extracting from semi-structured text in items descriptions a core set of properties and a popular subset of their ranges, then ex- tending the covered range using relational similarities in free text. Additionally, our process automatically tags the items with the new semantic information and exposes them as RDF triples. We present a set of experiments showing the effectiveness of our approach in this business context."
poster_45,45,"Corentin Jouault, Kazuhisa Seta and Yuki Hayashi",Jouault,SOLS: A Semantically Enriched Learning System Using LOD Based Automatic Question Generation,poster,Poster and Demo session,43,Corentin Jouault,poster_45.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,The purpose of this research is to use Linked Open Data (LOD) to support history learning on the Internet. The main issue to create meaningful content-dependent advice for learners is that the system requires an understanding of the learning domain. The learners use the Semantic Open Learning Space (SOLS) to create a machine-understandable concept map that represent their knowledge. SOLS is able to dynamically generate questions depending on each learner’s concept map. The system uses history domain ontologies to generate questions that aim to help learners develop their deep historical considerations. An evaluation showed that the learners using the question generation function could express deeper historical considerations after learning.
research_10,10,"Konrad Höffner, Jens Lehmann and Ricardo Usbeck",Höffner,CubeQA—Question Answering on RDF Data Cubes,research,Search (II),1,Konrad Höffner,research_10.pdf,504+505,2016-10-20T15:30:00,2016-10-20T15:50:00,"Statistical data in the form of RDF Data Cubes is becoming increasingly valuable as it influences decisions in areas such as health care, policy and finance. While a growing amount is becoming freely available through the open data movement, this data is opaque to laypersons. Semantic Question Answering (SQA) technologies provide access via free-form natural language queries but general SQA systems cannot process RDF Data Cubes. On the intersection between RDF Data Cubes and SQA, we create a new subfield of SQA, called RDCQA. We create an RDQCA benchmark as task 3 of the QALD-6 evaluation challenge, to stimulate further research and enable quantitative comparison between RDCQA systems. We design and evaluate the CubeQA algorithm, which
is the first RDCQA system and achieves a global F 1 score of 0.43 on the QALD6T3-test dataset, showing that RDCQA is feasible."
resource_69,69,"Daniel Hernandez, Aidan Hogan, Cristian Riveros, Carlos Rojas and Enzo Zerega",Hernandez,"Querying Wikidata: Comparing SPARQL, Relational and Graph Databases",resource,Querying/SPARQL (I),3,Daniel Hernandez,resource_69.pdf,501,2016-10-20T11:10:00,2016-10-20T11:30:00,"In this paper, we experimentally compare the efficiency of various database engines for the purposes of querying the Wikidata knowledge-base, which can be conceptualised as a directed edge-labelled where edges can be annotated with meta-information called qualifiers. We select two popular SPARQL databases (Virtuoso, Blazegraph), a popular relational database (PostgreSQL), and a popular graph database (Neo4J) for comparison and discuss various options as to how Wikidata can be represented in the models of each engine. We design a set of experiments to test the relative query performance of these representations in the context of their respective engines. We first execute a large set of atomic lookups to establish a baseline performance for each test setting, and subsequently perform experiments on instances of more complex graph patterns based on real-world examples. We conclude with a summary of the strengths and limitations of the engines observed."
resource_65,65,"Bijan Parsia, Nicolas Matentzoglu, Rafael S. Gonçalves, Birte Glimm and Andreas Steigmiller",Parsia,The OWL Reasoner Evaluation (ORE) 2015 Resources,resource,Reasoning,3,Bijan Parsia,resource_65.pdf,504+505,2016-10-21T16:10:00,2016-10-21T16:30:00,"The OWL Reasoner Evaluation (ORE) Competition is an annual competition (with an associated workshop) which pits OWL 2 compliant reasoners against each other on various standard reasoning tasks over naturally occurring problems. The 2015 competition was the third of its sort and had 14 reasoners competing in six tracks comprising three tasks (consistency, classification, and realisation) over two profiles (OWL 2 DL and EL). In this paper, we outline the design of the competition and present the infrastructure used for its execution: the corpora of ontologies, the competition framework, and the submitted systems. All resources are publicly available on the Web, allowing users to easily re-run the 2015 competition, or reuse any of the ORE infrastructure for reasoner experiments or ontology analysis."
resource_60,60,"Thomas Wilmering, György Fazekas and Mark B. Sandler",Wilmering,AUFX-O: Novel Methods for the Representation of Audio Processing Workflows,resource,Ontologies (II),4,Thomas Wilmering,resource_60.pdf,504+505,2016-10-21T14:30:00,2016-10-21T14:50:00,"This paper introduces the Audio Effects Ontology (AUFX-O) building on previous theoretical models describing audio processing units and workflows in the context of music production. We discuss important conceptualisations of different abstraction layers, their necessity to successfully model audio effects, and their application method. We present use cases concerning the application of effects in music production projects, and the creation of audio effect metadata facilitating a linked data service exposing information about effect implementations. By doing so, we show how our model benefits knowledge sharing, and enables reproducibility and analysis of audio production workflows."
resource_62,62,"Andrea Mauri, Jean-Paul Calbimonte, Daniele Dell'Aglio, Marco Balduini, Marco Brambilla, Emanuele Della Valle and Karl Aberer",Mauri,TripleWave: Spreading RDF Streams on the Web,resource,Streams,4,Andrea Mauri,resource_62.pdf,501,2016-10-20T14:30:00,2016-10-20T14:50:00,"Processing data streams is increasingly gaining a momentum, given the need to process these flows of information in real time and at Web scale. 
In this context, RDF Stream Processing (RSP) and Stream Reasoning (SR) have emerged as solutions to combine semantic technologies with stream and event processing techniques. 
Research in these areas has proposed an ecosystem of solutions to query, reason and perform real time processing over heterogeneous and distributed data streams on the Web.
However, so far one basic building block has been missing: a mechanism to disseminate and exchange RDF streams on the Web.
In this work we close this gap, proposing TripleWave, a reusable and generic tool that enables the publication of RDF streams on the Web. 
The features of TripleWave have been derived from requirements of real use-cases, and consider a diverse set of scenarios, independent of any specific RSP implementation.
TripleWave can be fed with existing Web streams (e.g. Twitter and Wikipedia streams) or time-annotated RDF datasets (e.g. the LinkedSensorData set), and it can be invoked through both pull- and push-based mechanisms, thus also enabling RSP engines to automatically register and receive data from TripleWave."
research_98,98,Konstantina Bereta and Manolis Koubarakis,Bereta,Ontop of Geospatial Databases,research,Querying/SPARQL (II),4,Konstantina Bereta,research_98.pdf,502,2016-10-21T14:30:00,2016-10-21T14:50:00,"In this paper we propose an OBDA approach for accessing geospatial data stored in geospatial relational databases, using the OGC standard GeoSPARQL and R2RML or OBDA mappings. We introduce extensions to existing SPARQL-to-SQL approaches to support GeoSPARQL features. We describe the implementation of our approach in the system ontop-spatial, an extension of the OBDA system Ontop for creating virtual geospatial RDF graphs on top of geospatial relational databases. Last, we present an experimental evaluation of our system using workload and queries from a recent benchmark. In order to measure the performance of our system, we compare it to the state-of-the-art geospatial RDF store, and confirm its efficiency."
journal_6,6,"Muhammad Saleem, Yasar Khan, Ali Hasnain, Ivan Ermilov and Axel-Cyrille Ngonga Ngomo",Saleem,A Fine-Grained Evaluation of SPARQL Endpoint Federation Systems,journal,Querying/SPARQL (I),4,Muhammad Saleem,journal_6.pdf,501,2016-10-20T11:30:00,2016-10-20T11:50:00,"The Web of Data has grown enormously over the last years. Currently, it comprises a large compendium of interlinked and distributed datasets from multiple domains. Running complex queries on this compendium often requires accessing data from different endpoints within one query. The abundance of datasets and the need for running complex query has thus motivated a considerable body of work on SPARQL query federation systems, the dedicated means to access data distributed over the Web of Data. However, the granularity of previous evaluations of such systems has not allowed deriving of insights concerning their behavior in different steps involved during federated query processing. In this work, we perform extensive experiments to compare state-of-the-art SPARQL endpoint federation systems using the comprehensive performance evaluation framework FedBench. In addition to considering the tradition query runtime as an evaluation criterion, we extend the scope of our performance evaluation by considering criteria, which have not been paid much attention to in previous studies. In particular, we consider the number of sources selected, the total number of SPARQL ASK requests used, the completeness of answers as well as the source selection time. Yet, we show that they have a significant impact on the overall query runtime of existing systems. Moreover, we extend FedBench to mirror a highly distributed data environment and assess the behavior of existing systems by using the same performance criteria. As the result we provide a detailed analysis of the experimental outcomes that reveal novel insights for improving current and future SPARQL federation systems."
journal_5,5,"Amrapali Zaveri, Anisa Rula, Andrea Maurino, Ricardo Pietrobon, Jens Lehmann and Sören Auer",Zaveri,Quality Assessment for Linked Data: A Survey,journal,Linked Data Measurement,4,Amrapali Zaveri,journal_5.pdf,504+505,2016-10-20T14:30:00,2016-10-20T14:50:00,"The development and standardization of semantic web technologies has resulted in an unprecedented volume of data being published on the Web as Linked Data (LD). However, we observe widely varying data quality ranging from extensively curated datasets to crowdsourced and extracted data of relatively low quality. In this article, we present the results of a systematic review of approaches for assessing the quality of LD. We gather existing approaches and analyze them qualitatively. In particular, we unify and formalize commonly used terminologies across papers related to data quality and provide a comprehensive list of 18 quality dimensions and 69 metrics. Additionally, we qualitatively analyze the 30 core approaches and 12 tools using a set of attributes. The aim of this article is to provide researchers and data curators a comprehensive understanding of existing work, thereby encouraging further experimentation and development of new approaches focused towards data quality, specifically for LD."
journal_4,4,"Diego Calvanese, Benjamin Cogrel, Sarah Komla-Ebri, Roman Kontchakov, Davide Lanti, Martin Rezk, Mariano Rodriguez-Muro and Guohui Xiao",Calvanese,Ontop: Answering SPARQL queries over relational databases,journal,Querying/SPARQL (II),3,Diego Calvanese,journal_4.pdf,502,2016-10-21T14:10:00,2016-10-21T14:30:00,"We present Ontop, an open-source Ontology-Based Data Access (OBDA) system that allows for querying relational data sources through a conceptual representation of the domain of interest, provided in terms of an ontology, to which the data sources are mapped. Key features of Ontop are its solid theoretical foundations, a virtual approach to OBDA, which avoids materializing triples and is implemented through the query rewriting technique, extensive optimizations exploiting all elements of the OBDA architecture, its compliance to all relevant W3C recommendations (including SPARQL queries, R2RML mappings, and OWL 2 QL and RDFS ontologies), and its support for all major relational databases."
journal_3,3,"Pierre-Yves Vandenbussche, Ghislain A. Atemezing, Maria Poveda and Bernard Vatant",Vandenbussche,Linked Open Vocabularies (LOV): a gateway to reusable semantic vocabularies on the Web,journal,Ontologies (I),1,Pierre-Yves Vandenbussche,journal_3.pdf,501,2016-10-20T15:30:00,2016-10-20T15:50:00,"One of the major barriers to the deployment of Linked Data is the difficulty that data publishers have in determining which vocabularies to use to describe the semantics of data. This system report describes Linked Open Vocabularies (LOV), a high quality catalogue of reusable vocabularies for the description of data on the Web. The LOV initiative gathers and makes visible indicators that have not been previously harvested such as the interconnections between vocabularies, version history along with past and current referent (individual or organization). The report details the various components of the system along with some innovations such as the introduction of a property-level boost in the vocabulary search scoring which takes into account the property's type (e.g rdfs:label, dc:comment) associated with a matching literal value. By providing an extensive range of data access methods (full-text search, SPARQL endpoint, API, data dump or UI), the project aims at facilitating the reuse of well-documented vocabularies in the Linked Data ecosystem. The adoption of LOV by many applications and methods shows the importance of such a set of vocabularies and related features for the ontology design activity and the publication of data on the Web."
journal_2,2,"Suvodeep Mazumdar, Daniela Petrelli, Khadija Elbedweihy, Vitaveska Lanfranchi and Fabio Ciravegna",Mazumdar,Affective Graphs: The Visual Appeal of Linked Data,journal,Interaction,4,Suvodeep Mazumdar,journal_2.pdf,504+505,2016-10-19T12:00:00,2016-10-19T12:20:00,"The essence and value of Linked Data lies in the ability of humans and machines to query, access and reason upon highly structured and formalised data. Ontology structures provide an unambiguous description of the structure and content of data. While a multitude of software applications and visualization systems have been developed over the past years for Linked Data, there is still a significant gap that exists between applications that consume Linked Data and interfaces that have been designed with significant focus on aesthetics. Though the importance of aesthetics in affecting the usability, effectiveness and acceptability of user interfaces have long been recognised, little or no explicit attention has been paid to the aesthetics of Linked Data applications. In this paper, we introduce a formalised approach to developing aesthetically pleasing semantic web interfaces by following aesthetic principles and guidelines identified from literature. We apply such principles to design and develop a generic approach of using visualizations to support exploration of Linked Data, in an interface that is pleasing to users. This provides users with means to browse ontology structures, enriched with statistics of the underlying data, facilitating exploratory activities and enabling visual query for highly precise information needs. We evaluated our approach in three ways: an initial objective evaluation comparing our approach with other well-known interfaces for the semantic web and two user evaluations with semantic web researchers."
journal_1,1,"Andrea Giovanni Nuzzolese, Valentina Presutti, Aldo Gangemi, Silvio Peroni and Paolo Ciancarini",Nuzzolese,Aemoo: Linked Data exploration based on Knowledge Patterns,journal,Search (II),3,Andrea Giovanni Nuzzolese,journal_1.pdf,504+505,2016-10-20T16:10:00,2016-10-20T16:30:00,"This paper presents a novel approach to Linked Data exploration that uses Encyclopedic Knowledge Patterns (EKPs) as relevance criteria for selecting, organising, and visualising knowledge. EKP are discovered by mining the linking structure of Wikipedia and evaluated by means of a user-based study, which shows that they are cognitively sound as models for building entity summarisations. We implemented a tool named Aemoo that supports EKP-driven knowledge exploration and integrates data coming from heterogeneous resources, namely static and dynamic knowledge as well as text and Linked Data. Aemoo is evaluated by means of controlled, task-driven user experiments in order to assess its usability, and ability to provide relevant and serendipitous information as compared to two existing tools: Google and RelFinder."
research_146,146,"Zlatan Dragisic, Valentina Ivanova, Patrick Lambrix, Daniel Faria, Ernesto Jiménez-Ruiz and Catia Pesquita",Dragisic,User validation in ontology alignment,research,Ontology Matching,3,Zlatan Dragisic,research_146.pdf,502,2016-10-20T11:10:00,2016-10-20T11:30:00,"User validation is one of the challenges facing the ontology alignment community, as there are limits to the quality of automated alignment algorithms.
In this paper we present a broad study on user validation of ontology alignments that encompasses three distinct but interrelated aspects: the profile of the user, the services of the alignment system, and its user interface. We discuss key issues pertaining to the alignment validation process under each of these aspects, and provide an overview of how current systems address them. Finally, we use experiments from the Interactive Matching track of the Ontology Alignment Evaluation Initiative (OAEI) 2015 to assess the impact of errors in alignment validation, and how systems cope with them as function of their services."
research_90,90,Lei Zhang and Achim Rettinger,Zhang,A Knowledge Base Approach to Cross-lingual Keyword Query Interpretation,research,Search (I),3,Lei Zhang,research_90.pdf,502,2016-10-20T14:10:00,2016-10-20T14:30:00,"The amount of entities in large knowledge bases available on the Web has been increasing rapidly, making it possible to propose new ways of intelligent information access. In addition, there is an impending need for technologies that can enable cross-lingual information access. As a simple and intuitive way of specifying information needs, keyword queries enjoy widespread usage, but suffer from the challenges including ambiguity, incompleteness and cross-linguality. In this paper, we present a knowledge base approach to cross-lingual keyword query interpretation by transforming keyword queries in different languages to their semantic representation, which can facilitate query disambiguation and expansion, and also bridge the language barriers of queries. The experimental results show that our approach achieves both high efficiency and effectiveness and considerably outperforms the baselines."
research_222,222,"Minh Pham, Suresh Alse, Craig Knoblock and Pedro Szekely",Pham,Semantic labeling: A domain-independent approach,research,Enriching Data Sources,3,Minh Pham,research_222.pdf,502,2016-10-21T16:10:00,2016-10-21T16:30:00,"Semantic labeling is the process of mapping attributes in data sources to classes in an ontology and is a necessary step in heterogeneous data integration. Variations in data formats, attribute names and even ranges of values of data make this a very challenging task. In this paper, we present a novel domain-independent approach to automatic semantic labeling that uses machine learning techniques. Previous approaches use machine learning to learn a model that extracts features related to the data of a domain, which requires the model to be re-trained for every new domain. Our solution uses similarity metrics as features to compare against labeled domain data and learns a matching function to infer the correct semantic labels for data. Since our approach depends on the learned similarity metrics but not the data itself, it is domain-independent and only needs to be trained once to work effectively across multiple domains. In our evaluation, our approach achieves higher accuracy than other approaches, even when the learned models are trained on domains other than the test domain."
research_94,94,Minh-Duc Pham and Peter Boncz,Pham,Exploiting Emergent Schemas to make RDF systems more efficient,research,Querying/SPARQL (II),2,Minh-Duc Pham,research_94.pdf,502,2016-10-21T13:50:00,2016-10-21T14:10:00,"We build on our earlier finding that more than 95% of the triples in actual RDF triple graphs have a remarkably tabular structure, whose schema does not necessarily follow from explicit metadata such as ontologies, but which an RDF store can automatically derive by looking at the data using so-called ``emergent schema'' detection techniques. In this paper we investigate how computers and in particular RDF stores can take advantage from this emergent schema to more compactly store RDF data and more efficiently optimize and execute SPARQL queries. To this end, we contribute techniques for efficient emergent schema aware RDF storage and new query operator algorithms for emergent schema aware scans and joins. In all, these techniques allow RDF schema processors fully catch up with relational database techniques in terms of rich physical database design options and efficiency, without requiring a rigid upfront schema structure definition."
journal_9,9,Danh Le-Phuoc,Le-Phuoc,Operator-aware approach for boosting performance in RDF stream processing,journal,Streams,3,Danh Le-Phuoc,journal_9.pdf,501,2016-10-20T14:10:00,2016-10-20T14:30:00,"To enable efficiency in stream processing, the evaluation of a query is usually performed over bounded parts of (potentially) unbounded streams, i.e., processing windows “slide” over the streams. To avoid inefficient re-evaluations of already evaluated parts of a stream in respect to a query, incremental evaluation strategies are applied, i.e., the query results are obtained incrementally from the result set of the preceding processing state without having to re-evaluate all input buffers. This method is highly efficient but it comes at the cost of having to maintain processing state, which is not trivial, and may defeat performance advantages of the incremental evaluation strategy. In the context of RDF streams the problem is further aggravated by the hard-to-predict evolution of the structure of RDF graphs over time and the application of sub-optimal implementation approaches, e.g., using relational technologies for storing data and processing states which incur significant performance drawbacks for graph-based query patterns. To address these performance problems, this paper proposes a set of novel operator-aware data structures coupled with incremental evaluation algorithms which outperform the counterparts of relational stream processing systems. This claim is demonstrated through extensive experimental results on both simulated and real datasets."
journal_8,8,"Marco Rospocher, Marieke van Erp, Piek Vossen, Antske Fokkens, Itziar Aldabe, German Rigau, Aitor Soroa, Thomas Ploeger, Tessel Bogaard",Rospocher,Building event-centric knowledge graphs from news,journal,Natural Language Processing,4,Marco Rospocher,journal_8.pdf,504+505,2016-10-21T11:30:00,2016-10-21T11:50:00,"Knowledge graphs have gained increasing popularity in the past couple of years, thanks to their adoption in everyday search engines. Typically, they consist of fairly static and encyclopedic facts about persons and organizations–e.g. a celebrity’s birth date, occupation and family members–obtained from large repositories such as Freebase or Wikipedia. In this paper, we present a method and tools to automatically build knowledge graphs from news articles. As news articles describe changes in the world through the events they report, we present an approach to create Event-Centric Knowledge Graphs (ECKGs) using state-of-the-art natural language processing and semantic web techniques. Such ECKGs capture long-term developments and histories on hundreds of thousands of entities and are complementary to the static encyclopedic information in traditional knowledge graphs. We describe our event-centric representation schema, the challenges in extracting event information from news, our open source pipeline, and the knowledge graphs we have extracted from four different news corpora: general news (Wikinews), the FIFA world cup, the Global Automotive Industry, and Airbus A380 airplanes. Furthermore, we present an assessment on the accuracy of the pipeline in extracting the triples of the knowledge graphs. Moreover, through an event-centered browser and visualization tool we show how approaching information from news in an event-centric manner can increase the user’s understanding of the domain, facilitates the reconstruction of news story lines, and enable to perform exploratory investigation of news hidden facts."""
demo_30,30,"Rivindu Perera, Parma Nand and Gisela Klette",Perera,Lexicalizing DBpedia with Realization Enabled Ensemble Architecture: RealText-lex2 Approach,demo,Poster and Demo session,49,Rivindu Perera,demo_30.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"DBpedia encodes massive amounts of open domain knowledge and is growing by accumulating more triples at the same rate as Wikipedia. However, in order to be able to present the knowledge processed using DBpedia, the applications need to present this knowledge often require natural language formulations of these triples. The RealText-lex2 framework offers a scalable platform to transform these triples to natural language sentences using lexicalization patterns. The framework has evolved from its previous version (RealText-lex) and is comprised of four lexicalization pattern mining modules which derive patterns from a training triple collection. These patterns can be then applied on the new triples given that they satisfy a defined set of constraints."
demo_31,31,"Joo Sungmin, Seiji Koide, Hideaki Takeda, Daisuke Horyu, Akane Takezaki and Tomokazu Yoshida",Sungmin,Agriculture Activity Ontology : An ontology for core vocabulary of agriculture activity,demo,Poster and Demo session,50,Joo Sungmin,demo_31.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"This paper proposes Agriculture Activity Ontology(AAO) as a basis of the core vocabulary of agricultural activity. Since concepts of agriculture activities are formed by the various context such as purpose, means, crop, and field, we organize the agriculture activity ontology as a hierarchy of concepts discriminated by various properties such as purpose, means, crop and field. The vocabulary of agricultural activity is then defined as the subset of the ontology. Since the ontology is consistent, extendable, and capable of some inferences thanks to Description Logics, so the vocabulary inherits these features. The vocabulary is also linked to existing vocabularies such as AGROVOC. It is expected to use in the data format in the agricultural IT system. The vocabulary is adopted as the part of ""the guideline for agriculture activity names for agriculture IT systems"" issued by Ministry of Agriculture, Forestry and Fisheries (MAFF), Japan."
demo_32,32,"Rivindu Perera, Parma Nand and Gisela Klette",Perera,Enriching Answers in Question Answering Systems using Linked Data,demo,Poster and Demo session,51,Rivindu Perera,demo_32.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Linked Data has emerged as the most widely used and the most powerful knowledge source for Question Answering (QA). Although Question Answering using Linked Data (QALD) fills in many gaps in the traditional QA models, the answers are still presented as factoids. This research introduces an answer presentation model for QALD by employing Natural Language Generation (NLG) to generate natural language descriptions to present an informative answer. The proposed approach employs lexicalization, aggregation, and referring expression generation to build a human-like enriched answer utilizing the triples extracted from the entities mentioned in the question as well as the entities contained in the answer."
demo_33,33,"Stasinos Konstantopoulos, Angelos Charalambidis, Giannis Mouchakis, Antonis Troumpoukis, Jürgen Jakobitsch and Vangelis Karkaletsis",Konstantopoulos,Semantic Web Technologies and Big Data Infrastructures: SPARQL Federated Querying of Heterogeneous Big Data Stores,demo,Poster and Demo session,52,Stasinos Konstantopoulos,demo_33.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The ability to cross-link large scale data with each other and with structured Semantic Web data, and the ability to uniformly process Semantic Web and other data adds value to both the Semantic Web and to the Big Data community. This paper presents work in progress towards integrating Big Data infrastructures with Semantic Web technologies, allowing for the cross-linking and uniform retrieval of data stored in both Big Data infrastructures and Semantic Web data. The technical challenges involved in achieving this, pertain to both data and system interoperability: we need a way to make the semantics of Big Data explicit so that they can interlink and we need a way to make it transparent for the client applications to query federations of such heterogeneous systems. The paper presents an extension of the Semagrow federated SPARQL query processor that is able to seamlessly federated SPARQL endpoints, Cassandra databases, and Solr databases, and discusses future directions of this line of work."
demo_34,34,"Kai Lenz, Hiroshi Masuya and Norio Kobayashi",Lenz,RIKEN MetaDatabase: a database publication platform for RIKENs life-science researchers that promotes research collaborations over different research area,demo,Poster and Demo session,53,Kai Lenz,demo_34.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"To promote data dissemination and integration of life science
datasets produced in a general research institute, RIKEN, we developed
an infrastructure database named as ""RIKEN MetaDatabase"", which en-
ables data publication and integration with Resource Description Frame-
work. We implemented simple data managing work 
ow, relational data-
base like graphical interface represents data links across laboratories. As
a result, activities of inter-laboratories collaborations and coordination
began to accelerated. Combined with global standardisation activities,
we expect this database can contribute data integration across the world."
demo_35,35,Adam Sotona,Sotona,How to feed Apache HBase with petabytes of RDF data: An extremely scalable RDF store based on Eclipse RDF4J framework and Apache HBase database,demo,Poster and Demo session,54,Adam Sotona,demo_35.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Eclipse RDF4J (formerly known as Sesame) is an open source Java framework for processing RDF data. RDF4J framework is extensible through its Storage And Inference Layer (SAIL) to support various RDF stores and inference engines. Apache HBase is the Hadoop database, a distributed and scalable big data store. It is designed to scale up from single servers to thousands of machines. We have connected RDF4J and HBase to receive an extremely scalable RDF store."
demo_36,36,"Wei Emma Zhang, Ermyas Abebe, Quan Z. Sheng and Kerry Taylor",Zhang,Towards Building Open Knowledge Base From Programming Question-Answering Communities,demo,Poster and Demo session,55,Wei Emma Zhang,demo_36.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper, we propose the rst system, so-called Open Programming Knowledge Extraction (OPKE), to automatically extract knowledge from programming Question-Answering (QA) communities. OPKE is the rst step of building a programming-centric knowledge base. Data mining and Natural Language Processing techniques are leveraged to identify paraphrased questions and construct structured information. Preliminary evaluation shows the eectiveness of OPKE."
demo_37,37,Gerhard Wohlgenannt and Filip Minic,Wohlgenannt,Using word2vec to Build a Simple Ontology Learning System,demo,Poster and Demo session,56,Gerhard Wohlgenannt,demo_37.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Ontology learning has been an important research area in the Semantic Web field in the last 20 years. Ontology learning systems generate domain models from data (typically text) using a combination of sophisticated methods. In this poster, we study the use of Google's word2vec to emulate a simple ontology learning system, and compare the results to an existing ""traditional"" ontology learning system.
"
resource_1,1,"Petar Ristoski, Gerben Klaas Dirk de Vries and Heiko Paulheim",Ristoski,A Collection of Benchmark Datasets for Systematic Evaluations of Machine Learning on the Semantic Web,resource,Embeddings & Neural Approaches,2,Petar Ristoski,resource_1.pdf,502,2016-10-20T15:50:00,2016-10-20T16:10:00,"In the recent years, several approaches for machine learning on the Semantic Web have been proposed. However, no extensive comparisons between those approaches have been undertaken, in particular due to a lack of publicly available, acknowledged benchmark datasets. In this paper, we present a collection of 22 benchmark datasets at different sizes, derived from existing Semantic Web datasets as well as from external classification and regression problems linked to datasets in the Linked Open Data cloud. Such a collection of datasets can be used to conduct qualitative performance testing and systematic comparisons of approaches."
demo_93,93,"Satoshi Kume, Hiroshi Masuya, Yosky Kataoka and Norio Kobayashi",Kume,Development of an Ontology for an Integrated Image Analysis Platform to enable Global Sharing of Microscopy Imaging Data,demo,Poster and Demo session,58,Satoshi Kume,demo_93.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Imaging data are fundamental to life sciences. We aimed to construct a microscopy ontology for an integrated metadata database of optical and electron microscopy images combined with various bio-entities. To realise this, we applied the Resource Description Framework (RDF) to an Open Microscopy Environment (OME) data model, which is the de facto standard to describe optical microscopy images and experimental data. We translated the XML-based OME metadata into the base concept of Web Ontology Language (OWL) as a trial of developing microscopy ontology. We describe the OWL-based ontology of microscopy imaging data and propose 18 upper-level concepts of ontology with missing concepts such as electron microscopy, phenotype data, biosample, and imaging conditions."
demo_90,90,"Yuting Song, Taisuke Kimura, Biligsaikhan Batjargal and Akira Maeda",Song,Cross-Language Record Linkage using Word Embedding driven Metadata Similarity Measurement,demo,Poster and Demo session,59,Yuting Song,demo_90.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Aiming to link the records that refer to the same entity across multiple databases in different languages, we address the mismatches of wordings between literal translations of metadata in source language and metadata in target language, which cannot be calculated by string-based measures. In this paper, we propose a method based on word embedding, which can capture the semantic similarity relationships among words. The effectiveness of this method is confirmed in linking the same records between Ukiyo-e (Japanese traditional woodblock printing) databases in Japanese and English. This method could be applied to other languages since it makes little assumption about languages."
demo_91,91,Robin Keskisärkkä,Keskisärkkä,Representing RDF Stream Processing Queries in RSP-SPIN,demo,Poster and Demo session,60,Robin Keskisärkkä,demo_91.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"A number of RDF Stream Processing (RSP) systems have been developed to support the processing of streaming Linked Data, however, due to the lack of a standardized query language they all provide different SPARQL extensions. The RSP Community Group is in the process of developing a standardized RSP query language (RSP-QL), which incorporates many of features of existing RSP language extensions. In this demo paper we describe how RSP-SPIN, a SPIN extension for representing RSP-QL queries, can be used to encapsulate RSP queries as RDF, forming a syntax agnostic representation that can be used to support serialization into multiple RSP language extensions. This could be useful, for example, to reduce the effort required to produce and maintain RSP benchmarks, since developers can focus on a single representation per query, rather than manually implementing and validating queries for several languages in parallel. "
demo_96,96,"Amna Basharat, Khaled Rasheed and I. Budak Arpinar",Basharat,Harnessing Crowds and Experts for Semantic Annotation of the Qur'an,demo,Poster and Demo session,61,Amna Basharat,demo_96.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper we illustrate how we harness the power of crowds and specialized experts through automated knowledge acquisition workflows for semantic annotation in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur'an, a widely studied manuscript, and apply a hybrid methodology of traditional 'crowdsourcing' augmented with 'expertsourcing' for semantically annotating its verses. We demonstrate that our proposed hybrid method presents a promising approach for achieving reliable annotations in an efficient and scalable manner, especially in cases where a high level of accuracy is required in knowledge intense and sensitive domains. "
resource_4,4,"Thomas Rebele, Fabian M. Suchanek, Johannes Hoffart, Joanna Biega, Erdal Kuzey and Gerhard Weikum",Rebele,"YAGO: a multilingual knowledge base from Wikipedia, Wordnet, and Geonames",resource,Multilinguality,1,Thomas Rebele,resource_4.pdf,501,2016-10-19T14:00:00,2016-10-19T14:20:00,"YAGO is a large knowledge base that is built automatically from Wikipedia, WordNet and GeoNames. The project combines information from 10 Wikipedias of different languages, thus giving the knowledge a multilingual dimension. It also attaches spatial and temporal information to many facts, and thus allows the user to query the data over space and time. YAGO focuses on extraction quality and achieves a manually evaluated precision of 95%. In this paper, we explain from a general perspective how YAGO is built from its sources, how its quality is evaluated, how a user can access it, and how other projects utilize it."
demo_94,94,Motoyuki Takaai and Yohei Yamane,Takaai,Constructing Semantic Networks of Development Activities from Weekly Reports,demo,Poster and Demo session,63,Motoyuki Takaai,demo_94.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In the development departments of some manufacturing companies,
there are weekly reports describing the status of events but they are poorly structured plain texts.
In this report, we propose a method for constructing semantic networks of development activities from weekly reports.
Our ontology-based method extracts things like events, status and agents from the reports and constructs relations
between them and creates Semantic MediaWiki pages from the semantic networks to visualize development activities.
We show a use case to apply the method to actual weekly reports and internal documents of a development department."
demo_9,9,"Pieter Heyvaert, Ruben Taelman, Ruben Verborgh, Erik Mannens and Rik Van de Walle",Heyvaert,Linked Sensor Data Generation using Queryable RML Mappings,demo,Poster and Demo session,100,Pieter Heyvaert,demo_9.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"As the amount of generated sensor data is increasing, semantic interoperability becomes an important aspect in order to support efficient data distribution and communication. Therefore, the integration and fusion of (sensor) data is important, as this data is coming from different data sources and might be in different formats. Furthermore, reusable and extensible methods for this integration and fusion are required in order to be able to scale with the growing number of applications that generate semantic sensor data. Current research efforts allow to map sensor data to Linked Data in order to provide semantic interoperability. However, they lack support for multiple data sources, hampering the integration and fusion. Furthermore, the used methods are not available for reuse or are not extensible, which hampers the development of applications. In this paper, we describe how the RDF Mapping Language (RML) and a Triple Pattern Fragments (TPF) server are used to address these shortcomings. %define reusable and extensible mappings to generate Linked Data based on heterogeneous (sensor) data. The demonstration consists of a micro controller that generates sensor data. The data is captured and mapped to RDF triples using module-specific RML mappings, which are queried from a TPF server."
demo_98,98,"Anastasia Dimou, Pieter Heyvaert, Wouter Maroy, Laurens De Graeve, Ruben Verborgh, Erik Mannens and Rik Van de Walle",Dimou,Towards an Interface for User-Friendly Linked Data Generation Administration,demo,Poster and Demo session,65,Anastasia Dimou,demo_98.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Linked Data generation and publication remain challenging and complicated, in particular for data owners who are not Semantic Web experts or tech-savvy. The situation deteriorates when data from multiple heterogeneous sources, accessed via different interfaces, is integrated, and the Linked Data generation is a long-lasting activity repeated periodically, often adjusted and incrementally enriched with new data. Therefore, we propose the RML Workbench, a graphical user interface to support data owners administrating their Linked Data generation and publication workflow. The RML Workbench’s underlying language is RML, since it allows to declaratively describe the complete Linked Data generation workflow. Thus, any Linked Data generation workflow specified by a user can be exported and reused by other tools interpreting RML."
demo_99,99,"Femke Ongenae, Pieter Bonte, Jelle Nelis, Thomas Vanhove and Filip De Turck",Ongenae,User-Friendly and Scalable Platform for the Design of Intelligent IoT Services: a Smart Office Use Case,demo,Poster and Demo session,66,Femke Ongenae,demo_99.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The Internet of Things (IoT) is starting to take a prevalent role in our daily lives. Smart offices that automatically adapt their environment to make life at the office as pleasant as possible, are slowly becoming reality.
In this paper we present a user-friendly semantic-based smart office platform that allows, through easy configuration, a personalized and comfortable experience at the office."
application_54,54,"Shusaku Egami, Takahiro Kawamura and Akihiko Ohsuga",Egami,Building Urban LOD for Solving Illegally Parked Bicycles in Tokyo,application,Smart Planet,2,Shusaku Egami,application_54.pdf,504+505,2016-10-19T14:20:00,2016-10-19T14:40:00,"The illegal parking of bicycles is a social problem in Tokyo and other urban areas. The purpose of this study was to sustainably build Linked Open Data (LOD) for the illegally parked bicycles and to support the problem solving by raising social awareness, in cooperation with the Bureau of General Affairs of Tokyo. We first extracted information on the problem factors and designed LOD schema for illegally parked bicycles. Then we collected pieces of data from Social Networking Service (SNS) and websites of municipalities to build the illegally parked bicycle LOD (IPBLOD) with more than 200,000 triples. We then estimated the missing data in the LOD based on the causal relations from the problem factors. As a result, the number of illegally parked bicycles can be inferred with 70.9% accuracy. Finally, we published the complemented LOD and a Web application to visualize the distribution of illegally parked bicycles in the city. We hope this raises social attention on this issue."
research_25,25,"Gong Cheng, Daxin Liu and Yuzhong Qu",Cheng,Efficient Algorithms for Association Finding and Frequent Association Pattern Mining,research,Data Mining,1,Gong Cheng,research_25.pdf,502,2016-10-21T10:30:00,2016-10-21T10:50:00,"Finding associations between entities is a common information need in many areas. It has been facilitated by the increasing amount of graph-structured data on the Web describing relations between entities. In this paper, we define an association connecting multiple entities in a graph as a minimal connected subgraph containing all of them. We propose an efficient graph search algorithm for finding associations, which prunes the search space by exploiting distances between entities computed based on a distance oracle. Having found a possibly large group of associations, we propose to mine frequent association patterns as a conceptual abstract summarizing notable subgroups to be explored, and present an efficient mining algorithm based on canonical codes and partitions. Extensive experiments on large, real RDF datasets demonstrate the efficiency of the proposed algorithms."
resource_50,50,"Heshan Du, Vania Dimitrova, Derek Magee, Anthony Cohn, Ross Stirling, Giulio Curioni, Barry Clarke and Helen Reeves",Du,An Ontology of Soil Properties and Processes,resource,Ontologies (II),2,Heshan Du,resource_50.pdf,504+505,2016-10-21T13:50:00,2016-10-21T14:10:00,"Assessing the Underworld (ATU) is a large interdisciplinary UK research project addressing urban infrastructure challenges, especially how to make streetworks more efficient and sustainable. One of the key challenges it addresses is integrated inter-asset maintenance. As the assets on the surface of the ground (e.g. pavements) and those buried under it (e.g. pipes and cables) are supported by the ground, the properties and processes of soil affect the performance of these assets to a significant degree. In order to make integrated decisions, it is necessary to combine the knowledge and expertise in multiple areas, such as roads, soil, buried assets, sensing, etc. This requires an underpinning knowledge model, in the form of an ontology. Within this context, we present a new ontology for describing soil properties (e.g. soil strength) and processes (e.g. soil compaction), as well as how they affect each other. This ontology can be used to express how the ground affects and is affected by assets buried under the ground or on the ground surface. The ontology is written in OWL 2 and openly available from the University of Leeds data repository: http://doi.org/10.5518/54."
research_21,21,Makoto Nakatsuji,Nakatsuji,Semantic Sensitive Simultaneous Tensor Factorization,research,Embeddings & Neural Approaches,3,Makoto Nakatsuji,research_21.pdf,502,2016-10-20T16:10:00,2016-10-20T16:30:00,"Semantics spread in large-scale knowledge bases can be used to intermediate heterogeneous users’ activity logs distributed in services; it can improve applications that assist users to decide next activities across services. Since user activities can be represented in terms of re- lationships involving three or more things (e.g. a user tags movie items on a webpage), they can be represented as a tensor. The recent semantic sensitive tensor factorization (SSTF) is promising since it achieves high accuracies in predicting users’ activities by applying semantics behind objects (e.g. item categories) to tensor factorization. However, SSTF fo- cuses on the factorization of data logs from a single service and thus has two problems: (1) the balance problem caused when simultaneously han- dling heterogeneous datasets and (2) the sparcity problem caused when there are insufficient data logs within a single service. Our solution, Se- mantic Sensitive Simultaneous Tensor Factorization (S3TF), tackles the above problems by: (1) It creates tensors for individual services and fac- torizes those tensors simultaneously; it does not force to create a tensor from multiple services and factorize the single tensor. This avoids low prediction results caused by the balance problem. (2) It utilizes shared semantics behind distributed logs and gives semantic biases to each ten- sor factorization. This avoids the sparsity problem by using the shared se- mantics among services. Experiments using the real-world datasets show that S3TF achieves up to 13% higher accuracy in rating predictions than the current best tensor method. It also extracts implicit relationships across services in the feature spaces by simultaneouse factorization."
demo_61,61,"Luca Costabello, Pierre-Yves Vandenbussche, Gofran Shukair, Corine Deliot and Neil Wilson",Costabello,Access Logs Don't Lie: Towards Traffic Analytics for Linked Data Publishers,demo,Poster and Demo session,80,Luca Costabello,demo_61.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Considerable investment in RDF publishing has recently led to the birth of the Web of Data. But is this investment worth it? Are publishers aware of how their linked datasets traffic looks like?
We propose an access analytics platform for linked datasets. The system mines traffic insights from the logs of registered RDF publishers and extracts Linked Data-specific metrics not available in traditional web analytics tools.
We present a demo instance showing one month (December 2014) of real traffic to the British National Bibliography RDF dataset."
resource_59,59,"Alo Allik, György Fazekas and Mark Sandler",Allik,Ontological representation of audio features,resource,Ontologies (II),3,Alo Allik,resource_59.pdf,504+505,2016-10-21T14:10:00,2016-10-21T14:30:00,"Feature extraction algorithms in Music Informatics aim at deriving statistical and semantic information directly from audio signals. These may be ranging from energies in several frequency bands to musical information such as key, chords or rhythm. There is an increasing diversity and complexity of features and algorithms in this domain and applications call for a common structured representation to facilitate interoperability, reproducibility and machine interpretability. We propose a solution relying on Semantic Web technologies that is designed to serve a dual purpose (1) to represent computational workflows of audio features and (2) to provide a common structure for feature data to enable the use of Open Linked Data principles and technologies in Music Informatics. The Audio Feature Ontology is based on the analysis of existing tools and music informatics literature, which was instrumental in guiding the ontology engineering process. The ontology provides a descriptive framework for expressing different conceptualisations of the audio feature extraction domain and enables designing linked data formats for representing feature data. In this paper, we discuss important modelling decisions and introduce a harmonised ontology library consisting of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and publishing. "
resource_58,58,"Gully Burns, Ulf Hermjakob and José Luis Ambite",Burns,Abstract Meaning Representations as Linked Data,resource,Natural Language Processing,2,Gully Burns,resource_58.pdf,504+505,2016-10-21T10:50:00,2016-10-21T11:10:00,"Significant advances in Natural Language Processing (NLP) research are fostered when high-quality annotated corpora were provided for general use. In an effort to develop a sembank (i.e., an annotated corpus dedicated to capturing the semantic meaning of a large set of annotated sentences), NLP researchers have developed the Abstract Meaning Representation (AMR) formulation. Each AMR is a rooted, labeled graph that represents the semantics of a single sentence. Nodes in the core AMR graph represent concepts/entities (such as nouns, PropBank frames, etc.) and edges represent relations between concepts, (such a frame-specific arguments, roles, etc.). AMRs have been used to annotate corpora of classic books, newstext and the biomedical research literature. Research is progressing on creating automatic parsers to generate AMRs directly from textual input. In the work described here, we map the AMR representation to a linked data format (AMR-LD), adopting the ontological formulation of the underlying AMR faithfully. We describe the process of generating AMR-LD data from standard AMRs derived from biomedical research articles, including mapping named entities to well-known linked-data resources, such as Uniprot and PubChem, as well as an open-source software to convert AMR data to RDF. We describe the benefits of AMR-LD, including convenient analysis using SPARQL queries and ontology inferences, and embedding into the web of Linked Data. Finally, we discuss the possible impact of semantic web representations that are directly derived from natural language."
research_29,29,"Eugene Siow, Thanassis Tiropanis and Wendy Hall",Siow,SPARQL-to-SQL on Internet of Things Databases and Streams,research,Streams,2,Eugene Siow,research_29.pdf,501,2016-10-20T13:50:00,2016-10-20T14:10:00,"To realise a semantic Web of Things, the challenge of achieving efficient Resource Description Format (RDF) storage and SPARQL query performance on Internet of Things (IoT) devices with limited resources has to be addressed. State-of-the-art SPARQL-to-SQL engines have been shown to outperform RDF stores on some benchmarks. In this paper, we describe an optimisation to the SPARQL-to-SQL approach, based on a study of time-series IoT data structures, that employs metadata abstraction and efficient translation by reusing existing SPARQL engines to produce Linked Data `just-in-time'. We evaluate our approach against RDF stores, state-of-the-art SPARQL-to-SQL engines and streaming SPARQL engines, in the context of IoT data and scenarios. We show that storage efficiency, with succinct row storage, and query performance can be improved from 2 times to 3 orders of magnitude."
resource_36,36,"Haofen Wang, Zhijia Fang, Jorge Gracia, Julia Bosque-Gil and Tong Ruan",Wang,Zhishi.lemon：On Publishing Zhishi.me as Linguistic Linked Open Data,resource,Multilinguality,3,Haofen Wang,resource_36.pdf,501,2016-10-19T14:40:00,2016-10-19T15:00:00,"Recently, a growing number of linguistic resources in different languages have been published and interlinked as part of the Linguistic Linked Open Data (LLOD) cloud. However, in comparison to English and other prominent languages, the presence of Chinese in such a cloud is still limited, despite the fact that Chinese is the most spoken language worldwide. Publishing more Chinese language resources in the LLOD cloud can benefit both academia and industry to better understand the language itself and to further build multilingual applications that will improve the flow of data and services across countries. In this paper, we describe Zhishi.lemon, a newly developed dataset based on the lemon model that constitutes the lexical realization of Zhishi.me, one of the largest Chinese datasets in the Linked Open Data (LOD) cloud. Zhishi.lemon combines the lemon core with the lemon translation module in order to build a linked data lexicon in Chinese with translations into Spanish and English. Links to BabelNet (a vast multilingual encyclopedic resource) have been provided as well. We also present a showcase of this module along with the technical details of transforming Zhishi.me to Zhishi.lemon. We have made the dataset accessible on the Web for both humans (via a Web interface) and software agents (with a SPARQL endpoint)."
poster_56,56,"Andrea Giovanni Nuzzolese, Anna Lisa Gentile, Valentina Presutti and Aldo Gangemi",Nuzzolese,Generating Conference Linked Open Data in One Click,poster,Poster and Demo session,67,Andrea Giovanni Nuzzolese,poster_56.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper we describe cLODg2 (conference Linked Open Data generator - version 2), a tool to collect, refine and produce Linked Data about scientific conferences with their associated publications, participants and events. Conference metadata collected from different unstructured and semi-structured resources must be expressed with appropriate vocabularies to be exposed as Linked Data. cLODg2 facilitates this task by providing a one-click workflow to generate data which is ready to be integrated in the ScholarlyData.org dataset. cLODg2 is an open source project, which has the aim to foster the publication of scholarly Linked Open Data and encourage collaborative efforts in this direction between researchers and publishers."
poster_57,57,"Ian Harrow, Martin Romacker, Andrea Splendiani, Stefan Negru, Peter Woollard, Scott Markel, Yasmin Alam-Faruque, Martin Koch, Erfan Younesi, James Malone and Ernesto Jimenez-Ruiz",Harrow,Ontologies Guidelines for Best Practice and a Process to Evaluate Existing Ontologies Mapping Tools and Algorithms,poster,Poster and Demo session,68,Ian Harrow,poster_57.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The Pistoia Alliance Ontologies Mapping project (http://www.pistoiaalliance.org/projects/ontologies-mapping) was set up to find or create better tools or services for mapping between ontologies in the same domain and to establish best practices for ontology management in the Life Sciences. It was proposed through the Pistoia Alliance Ideas Portfolio Platform (IP3: https://www.qmarkets.org/live/pistoia/home) which was selected by the Pistoia Alliance Operations Team for development of a formal business case.
The project has delivered a set of guidelines for best practice which build on existing standards. We show how these guidelines can be used as a ""checklist"" to support the application and mapping of source ontologies in the disease and phenotype domain. Another important output of this project was to specify the requirements for an Ontologies Mapping Tool. These requirements were used in a preliminary survey that established that such tools already exist which substantially meet them. Therefore, we have developed a formal process to define and submit a request for information (RFI) from existing ontologies mapping tool providers to enable their evaluation. This RFI process will be described and we summarise our findings from evaluation of seven ontologies mapping tools from academic and commercial providers. The guidelines and RFI materials are accessible on a public wiki:- https://pistoiaalliance.atlassian.net/wiki/display/PUB/Ontologies+Mapping+Resources.
A critical component of any Ontologies Mapping tool is the embedded ontology matching algorithm. Therefore, the Pistoia Alliance Ontologies Mapping Project is supporting development and evaluation of ontology matching algorithms though sponsorship and organisation of the new Disease and Phenotype track for OAEI 2016, which is also be summarised in this poster. This new track has been organised because currently, mappings between ontologies in a given data domain are mostly curated by bioinformatics and disease experts in academia or industry, who would benefit from automation of their procedures. This could be accomplished through implementation of ontology matching algorithms into their existing workflow environment or investment in an ontologies mapping tool for management of the ontologies mapping life cycle.
Work is in progress by the Ontologies Mapping project is to develop user requirements for an ontologies mapping service. We will conduct a survey of Pistoia Alliance members to understand the need for such a service and whether it should be implemented in future."
poster_55,55,"Dieter De Paepe, Ruben Verborgh, Erik Mannens and Rik Van de Walle",Paepe,Rule-Based Reasoning using State Space Search,poster,Poster and Demo session,69,Dieter De Paepe,poster_55.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Semantic Web reasoners are powerful tools that allow the extraction of implicit information from RDF data. This information is reachable through the definition of ontologies and/or rules provided to the reasoner. To achieve this, various algorithms are used by different reasoners. In this paper, we explain how state space search can be applied to perform backward-chaining rule-based reasoning. State space search is an approach used in the Artificial Intelligence domain that solves problems by modeling them as a graph and searching (using diverse algorithms) for solutions within this graph. State space search offers inherent proof generation and the ability to plug in different search algorithms to determine the characteristics of the reasoner such as: speed, memory or ensuring shortest proof generation."
poster_53,53,"Zhihui Liu, Xiaowang Zhang and Zhiyong Feng",Liu,Enhancing Rule-based OWL Reasoning on Spark,poster,Poster and Demo session,70,Zhihui Liu,poster_53.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The rule-based OWL reasoning is to compute the deductive
closure of an ontology by applying RDF/RDFS and OWL entailment
rules. In this paper, we present an approach to enhancing the perfor-
mance of the rule-based OWL reasoning on Spark based on a locally
optimal executable strategy. Firstly, we divide all rules (27 in total) in-
to four main classes, namely, SPO rules (5 rules), type rules (7 rules),
sameAs rules (7 rules), and schema rules (8 rules) since, as we investi-
gated, those triples corresponding to the rst three classes of rules are
overwhelming (e.g., over 99% in the LUBM dataset) in our practical
world. Secondly, based on the interdependence among those entailment
rules in each class, we pick out an optimal rule executable order of each
class and then combine them into a new rule execution order of all rules.
Finally, we implement the new rule execution order on Spark. The exper-
imental results show that the running time of our approach is improved
by about 30% as compared to Kim & Park's algorithm (2015)."
poster_50,50,Julien Subercaze and Christophe Gravier,Subercaze,Parallel sort-merge-join reasoning,poster,Poster and Demo session,71,Julien Subercaze,poster_50.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We present an in-memory, cross-platform, parallel reasoner
for RDFS and RDFSPlus . Inferray uses carefully optimized hash-based
join and sorting algorithms to perform parallel materialization. Designed
to take advantage of the architecture of modern CPUs, Inferray exhibits
a very good uses of cache and memory bandwidth. It offers state-of-the-
art performance on RDFS materialization, outperforms its counterparts
on RDFSPlus and can be connected with Jena.
Reasons to see the poster: i) Presentation of the system, how to use
it; ii) Discussion about implementation, source code walkthrough."
poster_51,51,Christophe Gravier and Julien Subercaze,Gravier,USE-RB : Benchmarking how reasoners work in harmony with modern hardware,poster,Poster and Demo session,72,Christophe Gravier,poster_51.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"As our computers embed more cores, efficient reasoners are designed with parallelization but also CPU and memory friendliness in mind. %
These latter contribute to make reasoner tractable in practice despite the computational complexity of logical fragments. %
However, creating benchmark to monitor this CPU-friendliness for many reasoners, datasets and logical fragments is a tedious task. %
In this paper, we present the Université Saint-Etienne Reasoners Benchmark (USE-RB) that automates the setup and execution of reasoners benchmarks with a particular attention to monitor how
reasoners work in harmony with the CPU.
"
research_232,232,Renzo Angles and Claudio Gutierrez,Angles,The multiset semantics of SPARQL patterns,research,Querying,1,Renzo Angles,research_232.pdf,502,2016-10-19T14:00:00,2016-10-19T14:20:00,"The paper determines the algebraic and logic structure produced by the multiset semantics of the core patterns of SPARQL. We prove that the fragment formed by AND, UNION, OPTIONAL, FILTER, MINUS and SELECT corresponds precisely to both, the intuitive multiset relational algebra (projection, selection, natural join, arithmetic union and except), and multiset classical non-recursive Datalog with safe negation."
application_6,6,"Christian Hennig, Alexander Viehl, Benedikt Kämpgen and Harald Eisenmann",Hennig,Ontology-Based Design of Space Systems,application,Ontologies (I),4,Christian Hennig,application_6.pdf,501,2016-10-20T16:30:00,2016-10-20T16:50:00,"In model-based systems engineering a model specifying the system's design is shared across a variety of disciplines and used to ensure the consistency and quality of the overall design. Existing implementations for describing these system models exhibit a number of shortcomings regarding their approach to data management. In this emerging applications paper, we present the application of an ontology for space system design providing increased semantic soundness of the underlying standardized data specification, enabling reasoners to identify problems in the system, and allowing the application of operational knowledge collected over past projects to the system to be designed. Based on a qualitative evaluation driven by data derived from an actual satellite design project, a reflection on the applicability of ontologies in the overall model-based systems engineering approach is pursued."
journal_12,12,"Marcelo Arenas, Bernardo Cuenca Grau, Evgeny Kharlamov, Šarūnas Marciuška and Dmitriy Zheleznyakov",Arenas,Faceted search over RDF-based knowledge graphs,journal,Search (II),2,Marcelo Arenas,journal_12.pdf,504+505,2016-10-20T15:50:00,2016-10-20T16:10:00,"Knowledge graphs such as Yago and Freebase have become a powerful asset for enhancing search, and are being intensively used in both academia and industry. Many existing knowledge graphs are either available as Linked Open Data, or they can be exported as RDF datasets enhanced with background knowledge in the form of an OWL 2 ontology. Faceted search is the de facto approach for exploratory search in many online applications, and has been recently proposed as a suitable paradigm for querying RDF repositories. In this paper, we provide rigorous theoretical underpinnings for faceted search in the context of RDF-based knowledge graphs enhanced with OWL 2 ontologies. We identify well-defined fragments of SPARQL that can be naturally captured using faceted search as a query paradigm, and establish the computational complexity of answering such queries. We also study the problem of updating faceted interfaces, which is critical for guiding users in the formulation of meaningful queries during exploratory search. We have implemented our approach in a fully-fledged faceted search system, SemFacet, which we have evaluated over the Yago knowledge graph."
journal_11,11,"Daniel Gerber, Diego Esteves, Jens Lehmann, Lorenz Bühmann, Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo and René Speck",Gerber,DeFacto - Temporal and multilingual Deep Fact Validation,journal,Knowledge Graph,3,Daniel Gerber,journal_11.pdf,501,2016-10-19T11:40:00,2016-10-19T12:00:00,"One of the main tasks when creating and maintaining knowledge bases is to validate facts and provide sources for them in order to ensure correctness and traceability of the provided knowledge. So far, this task is often addressed by human curators in a three-step process: issuing appropriate keyword queries for the statement to check using standard search engines, retrieving potentially relevant documents and screening those documents for relevant content. The drawbacks of this process are manifold. Most importantly, it is very time-consuming as the experts have to carry out several search processes and must often read several documents. In this article, we present DeFacto (Deep Fact Validation)—an algorithm able to validate facts by finding trustworthy sources for them on the Web. DeFacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of web pages as well as useful additional information including a score for the confidence DeFacto has in the correctness of the input fact. To achieve this goal, DeFacto collects and combines evidence from web pages written in several languages. In addition, DeFacto provides support for facts with a temporal scope, i.e., it can estimate in which time frame a fact was valid. Given that the automatic evaluation of facts has not been paid much attention to so far, generic benchmarks for evaluating these frameworks were not previously available. We thus also present a generic evaluation framework for fact checking and make it publicly available."
journal_10,10,"Guillermo Vega-Gorgojo, Laura Slaughter, Martin Giese, Simen Heggestøyl, Ahmet Soylu and Arild Waaler",Vega-Gorgojo,Visual query interfaces for semantic datasets: an evaluation study,journal,Interaction,3,Guillermo Vega-Gorgojo,journal_10.pdf,504+505,2016-10-19T11:40:00,2016-10-19T12:00:00,"The rapid growth of the Linked Open Data cloud, as well as the increasing ability to lift relational enterprise datasets to a semantic, ontology-based level means that vast amounts of information are now available in a representation that closely matches the conceptualizations of the potential users of this information. This makes it interesting to create ontology based, user-oriented tools for searching and exploring this data. Although initial efforts were intended for tech users with knowledge of SPARQL/RDF, there are ongoing proposals designed for lay users. One of the most promising approaches is to use visual query interfaces, but more user studies are needed to assess their effectiveness. In this paper, we compare the effect on usability of two important paradigms for ontology-based query interfaces: form-based and graph-based interfaces. In order to reduce the number of variables affecting the comparison, we performed a user study with two state-of-the-art query tools developed by ourselves, sharing a large part of the code base: the graph-based tool OptiqueVQS*, and the form-based tool PepeSearch. We evaluated these tools in a formal comparison study with 15 participants searching a Linked Open Data version of the Norwegian Company Registry. Participants had to respond to 6 non-trivial search tasks using alternately OptiqueVQS* and PepeSearch. Even without previous training, retrieval performance and user confidence were very high, thus suggesting that both interface designs are effective for searching RDF datasets. Expert searchers had a clear preference for the graph-based interface, and mainstream searchers obtained better performance and confidence with the form-based interface. While a number of participants spontaneously praised the capability of the graph interface for composing complex queries, our results evidence that graph interfaces are difficult to grasp. In contrast, form interfaces are more learnable and relieve problems with disorientation for mainstream users. We have also observed positive results introducing faceted search and dynamic term suggestion in semantic search interfaces."
research_139,139,"Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani and Gerhard Weikum",Gad-Elrab,Exception-enriched Rule Learning from Knowledge Graphs,research,Data Mining,4,Mohamed H. Gad-Elrab,research_139.pdf,502,2016-10-21T11:30:00,2016-10-21T11:50:00,"Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, Yago and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. 
To overcome this problem, we present a method for effective revision of learned Horn rules by effectively incorporating exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction."
demo_92,92,"Md. Kamruzzaman Sarker, David Carral, Adila A. Krisnadhi and Pascal Hitzler",Sarker,Modeling OWL with Rules: The ROWL Protege Plugin,demo,Poster and Demo session,57,Md. Kamruzzaman Sarker,demo_92.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In our experience, some ontology modelers find it much easier to express logical axioms using rules rather than using OWL (or description logic) syntax. Based on recent theoretical developments on transformations between rules and description logics, we develop ROWL, a Protege plugin that allows users to enter OWL axioms by way of rules; the plugin then automatically converts these rules into OWL DL axioms if possible, and prompts the user in case such a conversion is not possible without weakening the semantics of the rule.
"
research_38,38,"Duhai Alshukaili, Alvaro A. A. Fernandes and Norman Paton",Alshukaili,Structuring Linked Data Search Results Using Probabilistic Soft Logic,research,Search (I),2,Duhai Alshukaili,research_38.pdf,502,2016-10-20T13:50:00,2016-10-20T14:10:00,"On-the-fly generation of integrated representations of Linked Data (LD)
 search results is challenging because it requires successfully automating a number
 of complex subtasks, such as structure inference and matching of
 both instances and concepts, each of which gives rise to uncertain
 outcomes. Such uncertainty is unavoidable given the semantically heterogeneous
 nature of web sources, including LD ones. This paper approaches the problem of 
 structuring LD search results as an evidence-based one. In particular, the paper shows 
 how one formalism (viz., probabilistic soft logic (PSL)) can be exploited to assimilate
 different sources of evidence in a principled way and to beneficial
 effect for users. The paper considers syntactic evidence derived from matching
 algorithms, semantic evidence derived from LD vocabularies, and user
 evidence, in the form of feedback. The main contributions are: sets 
 of PSL rules that model the uniform assimilation of diverse kinds of evidence, 
 an empirical evaluation of how the resulting PSL programs perform in terms 
 of their ability to infer structure in LD search results, and, finally, a concrete
 example of how populating such inferred structures for presentation
 to the end user is beneficial, besides enabling the collection of
 feedback whose assimilation further improves search result presentation."
demo_69,69,"Pankesh Patel, Amelie Gyrard, Dhavalkumar Thakker, Amit Sheth and Martin Serrano",Patel,SWoTSuite: A Toolkit for Prototyping Cross-domain Semantic Web of Things Applications,demo,Poster and Demo session,73,Pankesh Patel,demo_69.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Semantic Web of Things(SWoT) applications focus on providing a wide-scale interoperability that allows the sharing of IoT devices across domains and the reusing of available knowledge on the web. However, the application development is difficult because developers have to do various tasks such as designing an application, annotating IoT data, interpreting data, and combining application domains. 

To address the above challenges, this paper demonstrates SWoTSuite, a toolkit for prototyping SWoT applications. It hides the use of semantic web technologies as much as possible to avoid the burden of designing SWoT applications that involves designing ontologies, annotating sensor data, and using reasoning mechanisms to enrich data. Taking inspiration from sharing and reuse approaches, SWoTSuite reuses data and vocabularies. It leverages existing technologies to build applications. We take a hello world naturopathy application as an example and demonstrate an application development process using SWoTSuite. The demo video is available at URL- http://tinyurl.com/zs9flrt."
demo_68,68,"Damien Graux, Louis Jachiet, Pierre Geneves and Nabil Layaida",Graux,SPARQLGX in Action: Efficient Distributed Evaluation of SPARQL with Apache Spark,demo,Poster and Demo session,74,Damien Graux,demo_68.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We demonstrate SPARQLGX: our implementation of a distributed sparql evaluator. We show that SPARQLGX makes it possible to evaluate SPARQL queries on billions of triples distributed across multiple nodes, while providing attractive performance figures."
demo_67,67,"Valeria Fionda, Melisachew Wudage Chekol and Giuseppe Pirrò",Fionda,Gize: A Time Warp in the Web of Data,demo,Poster and Demo session,75,Valeria Fionda,demo_67.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We introduce the Gize framework for querying historical RDF data. Gize builds upon two main pillars: a lightweight approach to keep historical data, and an extension of SPARQL called SPARQ–LTL, which incorporates temporal logic primitives to enable a rich class of queries. One striking point of Gize is that its features can be readily made available in existing query processors."
demo_66,66,"Hassan Saif, Miriam Fernandez, Matthew Rowe and Harith Alani",Saif,On the Role of Semantics for Detecting pro-ISIS Stances on Social Media,demo,Poster and Demo session,76,Hassan Saif,demo_66.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting pro-ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic-based approach for radicalisation detection on Twitter. Unlike most previous works, which mainly rely on the lexical and contextual representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from words' semantics outperform those trained from lexical and network features by 2% on average F1-measure.
"
demo_65,65,"Jonas Bulegon Gassen, Stefano Faralli, Simone Paolo Ponzetto and Jan Mendling",Gassen,Who-Does-What: A knowledge base of people's occupations and job activities,demo,Poster and Demo session,77,Jonas Bulegon Gassen,demo_65.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"System analysis and design is concerned with the creation of conceptual models. In this paper, we introduce a novel resource called ""Who-Does-What"" (WDW) that supports the creation and quality assurance of such models. WDW provides a knowledge base of activities for classes of people engaged in a wide range of different occupations. The resource is semi-automatically created by populating the manually-created Standard Occupational Classification (SOC) of the US Department of Labor with activities found on the Web."
demo_64,64,"Changlong Wang, Xiaowang Zhang and Zhiyong Feng",Wang,Structure-guiding Modular Reasoning for Expressive Ontologies,demo,Poster and Demo session,78,Changlong Wang,demo_64.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We propose a technique that combine an OWL 2 EL reasoner with an OWL 2 reasoner to classify expressive ontologies. We exploit the information implied by the ontology structure to identify a small non-EL ontology that contains necessary axioms to ensure the completeness. In the process of ontology classification, the bulk of workload is delegated to an efficient OWL 2 EL reasoner and the small part of workload is handled by a less efficient OWL 2 reasoner. Experimental results show that our approach leads to a reasonable task assignment and offers a substantial speedup in ontology classification."
demo_62,62,Michał Blinkiewicz and Jaroslaw Bak,Blinkiewicz,SQuaRE: A Visual Tool For Creating R2RML Mappings,demo,Poster and Demo session,79,Michał Blinkiewicz,demo_62.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We present the recent progress of SQuaRE, the SPARQL
Query and R2RML mappings Environment which provides a graphical
interface for creating R2RML mappings which can be immediately tested
by executing SPARQL queries. SQuaRE is a web-based tool with easy
to use interface that can be applied in the ontology-based data access
applications. We describe SQuaRE’s main features, its architecture as
well as technical details."
application_47,47,"Gregoire Burel, Lara S. G. Piccolo and Harith Alani",Burel,EnergyUse - A Collaborative Semantic Platform for Monitoring and Discussing Energy Consumption,application,Smart Planet,3,Gregoire Burel,application_47.pdf,504+505,2016-10-19T14:40:00,2016-10-19T15:00:00,"Conserving fossil-based energy to reduce carbon emissions is key to slowing down global warming. The 2015 Paris agreement on climate change emphasised the importance of raising public awareness and participation to address this societal challenge. In this paper we introduce EnergyUse; a social and collaborative platform for raising awareness on climate change, by enabling users to view and compare the actual energy consumption of various appliances, and to share and discuss energy conservation tips in an open and social environment. The platform collects data from smart plugs, and exports appliance consumption information and community generated energy tips as linked data. In this paper we report on the system design, data modelling, platform usage and early deployment with a set of 58 initial participants. We also discuss the challenges, lessons learnt, and future platform developments."
demo_60,60,Thomas Wilmering and Mark B. Sandler,Wilmering,Interdisciplinary Classification of Audio Effects in the Audio Effect Ontology,demo,Poster and Demo session,81,Thomas Wilmering,demo_60.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"This paper discusses an extension to the Audio Effect Ontology (AUFX-O) for the interdisciplinary classification of audio effect types. The ontology extension implements a unified classification system that draws on knowledge from different music-related disciplines and is designed to facilitate the retrieval of audio effect information based on low-level and semantic aspects. It extends AUFX-O enabling communication between agents from different disciplines within the field of music creation and production. After briefly discussing the ontology, we show how it can be used to efficiently classify and retrieve effect types."
resource_46,46,"Damien Graux, Louis Jachiet, Pierre Geneves and Nabil Layaida",Graux,SPARQLGX: Efficient Distributed Evaluation of SPARQL with Apache Spark,resource,Querying/SPARQL (I),2,Damien Graux,resource_46.pdf,501,2016-10-20T10:50:00,2016-10-20T11:10:00,"SPARQL is the W3C standard query language for querying data expressed in the Resource Description Framework (RDF). The increasing amounts of RDF data available raise a major need and research interest in building efficient and scalable distributed SPARQL query evaluators. In this context, we propose and share SPARQLGX: our implementation of a distributed RDF datastore based on Apache Spark. SPARQLGX is designed to leverage existing Hadoop infrastructures for evaluating SPARQL queries. SPARQLGX relies on a translation of SPARQL queries into executable Spark code that adopts evaluation strategies according to (1) the storage method used and (2) statistics on data. We show that SPARQLGX makes it possible to evaluate SPARQL queries on billions of triples distributed across multiple nodes, while providing attractive performance figures. We report on experiments which show how SPARQLGX compares to related state-of-the-art implementations. Using a simple design, SPARQLGX already represents an interesting alternative in several scenarios. We share it as a resource for the further construction of efficient SPARQL evaluators."
resource_45,45,"Monika Solanki, Bojan Božić, Markus Freudenberg, Rob Brennan and Dimitris Kontokostas",Solanki,Enabling combined software and data engineering at Web-scale: The ALIGNED suite of ontologies,resource,Ontologies (II),1,Monika Solanki,resource_45.pdf,504+505,2016-10-21T13:30:00,2016-10-21T13:50:00,"Effective, collaborative integration of software services and big data to develop insightful analytics, for Web-scale systems, is now a crucial techno-economic challenge. This requires new combined data and software engineering processes and tools. Semantic metadata standards such as RDFS and OWL, and linked data principles, provide a technical grounding for such integrated systems given an appropriate model of the domain. In this paper we introduce the ALIGNED suite of ontologies or vocabularies specifically designed to model the information exchange needs of combined software and data engineering processes. The models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in trial live web-scale, data-intensive system development environments in both the commercial and academic domains. We exemplify the usage of the suite on a complex collaborative software and data engineering scenario from the legal information system domain."
poster_110,110,"Ben De Meester, Anastasia Dimou, Ruben Verborgh, Erik Mannens and Rik Van de Walle",Meester,Discovering and Using Functions via Content Negotiation,poster,Poster and Demo session,82,Ben De Meester,poster_110.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Data has been made reusable and machine-interpretable by publishing it as Linked Data.
However, Linked Data automatic processing is not fully achieved yet,
as manual effort is still needed to integrate existing tools and libraries within a certain technology stack.
To enable automatic processing,
we propose exposing functions and methods as Linked Data,
publishing it in different programming languages,
using content negotiation to cater to different technology stacks,
and making use of common, technology-independent identifiers to make them discoverable.
As such, we can enable automatic processing of Linked Data across formats and technology stacks.
By using discovery endpoints, similarly as being used to discover vocabularies and ontologies,
the publication of these functions can remain decentralized whilst still be easily discoverable."
resource_48,48,"Giuseppe Loseto, Saverio Ieva, Filippo Gramegna, Michele Ruta, Floriano Scioscia and Eugenio Di Sciascio",Loseto,Linked Data (in resourceless) Platforms: a mapping for Constrained Application Protocol,resource,Knowledge Graph,4,Giuseppe Loseto,resource_48.pdf,501,2016-10-19T12:00:00,2016-10-19T12:20:00,"This paper proposes a mapping of the Linked Data Platform (LDP) specification for Constrained Application Protocol (CoAP). Main motivation stems from the fact that LDP W3C Recommendation presents resource management primitives for HTTP only. Hence, use cases related to Web of Things scenarios, where HTTP-based communication and infrastructures are unfeasible, are partially neglected. A general translation of LDP-HTTP requests and responses is provided, as well as a fully comprehensive framework for HTTP-to-CoAP proxying. The theoretical work is corroborated by an experimental campaign using the W3C Test Suite for LDP."
demo_97,97,"Anastasia Dimou, Dimitris Kontokostas, Markus Freudenberg, Ruben Verborgh, Jens Lehmann, Erik Mannens, Sebastian Hellmann and Rik Van de Walle",Dimou,DBpedia Mappings Quality Assessment,demo,Poster and Demo session,62,Anastasia Dimou,demo_97.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The root of schema violations for RDF data generated from (semi-)structured data, often derives from mappings, which are repeatedly applied and specify how an RDF dataset is generated. The DBpedia dataset, which derives from Wikipedia infoboxes, is no exception. To mitigate the violations, we proposed in previous work to validate the mappings which generate the data, instead of validating the generated data afterwards. In this work, we demonstrate how mappings validation is applied to DBpedia. DBpedia mappings are automatically translated to RML and validated by RDFUnit. The DBpedia mappings assessment can be frequently executed, because it requires significantly less time compared to validating the dataset. The validation results become available via a user-friendly interface. The DBpedia community considers them to refine the DBpedia mappings or ontology and thus, increase the dataset quality."
doctoralconsortium_17,17,Jinhyun Ahn,Ahn,Optimization Techniques for 2-hop Labeling of Dynamic Directed Acyclic Graphs,doctoralconsortium,Session 2,3,Jinhyun Ahn,doctoralconsortium_17.pdf,,2016-10-18T14:30:00,2016-10-18T14:45:00,"Directed Acyclic Graph(DAG) data is increasingly available on the Web, including Linked Open Data(LOD). Mining reachability relationships between entities is an important task for extracting knowledge from LOD. Diverse labeling schemes have been proposed to efficiently determine the reachability. We focus on a state-of-the-art 2-hop labeling scheme that is based on a permutation of vertices to achieve a linear index size and reduce on-line searches that are required when the reachability cannot be answered by 2-hop labels only. We observed that the approach can be improved in three different ways; 1) space-efficiency - guarantee the minimized index size without randomness 2) update-efficiency - update labels efficiently when graphs changes 3) parallelization - labeling should be cluster-based, and solved in a distributed fashion. In these regards, this PhD thesis proposes optimization techniques that address these issues. In this paper in particular, a way of reducing the 2-hop label size is proposed with preliminary results on real-world DAG datasets. In addition, we will discuss the feasibilities of the other issues based on our on-going works."
research_202,202,"David Carral, Cristina Feier and Pascal Hitzler",Carral,A Practical Acyclicity Notion for Query Answering over Horn-SRIQ Ontologies,research,Reasoning,1,David Carral,research_202.pdf,504+505,2016-10-21T15:30:00,2016-10-21T15:50:00,"Conjunctive query answering over expressive Horn Description Logic ontologies is a relevant and challenging problem which, in some cases, can be addressed by application of the chase algorithm.
In this paper, we define a novel acyclicity notion which provides sufficient condition for termination of the restricted chase over Horn-SRIQ ontologies.
We show that our notions generalize most of the existing acyclicity conditions (both theoretically and empirically) and its use results in a more efficient reasoning procedure.
Furthermore, we implement a materialization based reasoner for acyclic ontologies which vastly outperforms state-of-the-art reasoners.
"
doctoralconsortium_11,11,Yordan Terziev,Terziev,Feature Generation using Ontologies during Induction of Decision Trees on Linked Data,doctoralconsortium,Session 2,2,Yordan Terziev,doctoralconsortium_11.pdf,,2016-10-18T14:15:00,2016-10-18T14:30:00,"Linked data has the potential of interconnecting data from different domains, bringing new potentials to machine agents to provide better services for web users. The ever increasing amount of linked data in government open data, social linked data, linked medical and patients’ data provides new opportuni-ties for data mining and machine learning. Both are however strongly de-pendent on the selection of high quality data features to achieve good results. In this work we present an approach that uses ontological knowledge to gen-erate features that are suitable for building a decision tree classifier address-ing the specific data set and classification problem. The approach that we present has two main characteristics - it generates new features on demand as required by the induction algorithm and uses ontological knowledge about linked data to restrict the set of possible options. These two characteristics enable the induction algorithm to look for features that might be connected through many entities in the linked data enabling the generation of cross-domain explanation models."
research_200,200,"Sebastian Neumaier, Jürgen Umbrich, Josiane Xavier Parreira and Axel Polleres",Neumaier,Multi-level semantic labelling of numerical values,research,Enriching Data Sources,1,Sebastian Neumaier,research_200.pdf,502,2016-10-21T15:30:00,2016-10-21T15:50:00,"With the success of Open Data a huge amount of tabular data sources
became available that could potentially be mapped and linked into the Web of
(Linked) Data. Most existing approaches to “semantically label” such tabular
data rely on mappings of textual information to classes, properties, or instances
in RDF knowledge bases in order to link – and eventually transform – tabular
data into RDF. However, as we will illustrate, Open Data tables typically contain
a large portion of numerical columns and/or non-textual headers; therefore
solutions that solely focus on textual “cues” are only partially applicable for mapping
such data sources. We propose an approach to find and rank candidates of
semantic labels and context descriptions for a given bag of numerical values. To
this end, we apply a hierarchical clustering over information taken from DBpedia
to build a background knowledge graph of possible “semantic contexts” for
bags of numerical values, over which we perform a nearest neighbour search to
rank the most likely candidates. Our evaluation shows that our approach can assign
fine-grained semantic labels, when there is enough supporting evidence in
the background knowledge graph. In other cases, our approach can nevertheless
assign high level contexts to the data, which could potentially be used in combination
with other approaches to narrow down the search space of possible labels."
demo_22,22,John P. Mccrae,Mccrae,Yuzu: Publishing Any Data as Linked Data,demo,Poster and Demo session,33,John P. Mccrae,demo_22.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Linked data is one of the most important methods for improving the applicability of data, however most data is not in linked data formats and raising it to linked data is still a significant challenge. We present Yuzu, an application that makes it easy to host legacy data in JSON, XML or CSV as linked data, while providing a clean interface with advanced features. The ease-of-use of this framework is shown by its adoption for a number of existing datasets including WordNet.
"
demo_29,29,"Evgeny Kharlamov, Bernardo Cuenca Grau, Ernesto Jimenez-Ruiz, Steffen Lamparter, Gulnar Mehdi, Martin Ringsquandl, Yavor Nenov, Stephan Grimm, Mikhail Roshchin and Ian Horrocks",Kharlamov,SOMM: Industry Oriented Ontology Management Tool,demo,Poster and Demo session,38,Evgeny Kharlamov,demo_29.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this demo we present the SOMM system that resulted from an ongoing collaboration between Siemens and the University of Oxford. The goal of this collaboration is to facilitate design and management of ontologies that capture conceptual information models underpinning various industrial applications. SOMM supports engineers with little background on semantic technologies in the creation of such ontologies and in populating them with data. SOMM implements a fragment of OWL 2 RL extended with a form of integrity constraints for data validation, and it comes with support for schema and data reasoning, as well as for ontology integration. We demonstrate functionality of SOMM on two scenarios from energy and manufacturing domains."
demo_95,95,"Atsuko Yamaguchi, Kouji Kozaki, Kai Lenz, Yasunori Yamamoto, Hiroshi Masuya and Norio Kobayashi",Yamaguchi,Data Acquisition by Traversing Class-Class Relationships over the Linked Open Data,demo,Poster and Demo session,64,Atsuko Yamaguchi,demo_95.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Linked Open Data (LOD) is a powerful mechanism for linking different datasets published on the Web, which is expected to create new value of data through mash-up over various datasets on the Web. One of the important needs to obtain data from LOD is to find a path of resources connecting given two classes, each of which has an end resource of the path. 
In this study, the two technologies for the approach are introduced: a labeled multi graph named class graph to compute class-class relationships and an RDF specification named SPARQL Builder Metadata to obtain and store required metadata for construction of a class graph. In addition, as a practical application, we introduce the SPARQL Builder system, which assists users in writing semantic queries for LOD."
doctoralconsortium_18,18,Radityo Eko Prasojo,Prasojo,Entity-Relationship Extraction from Wikipedia Unstructured Text ,doctoralconsortium,Session 1,1,Radityo Eko Prasojo,doctoralconsortium_18.pdf,,2016-10-18T11:00:00,2016-10-18T11:15:00,"Wikipedia has been the primary source of information for many automatically-generated Semantic Web data sources. However, they suffer from incompleteness since they largely do not cover information contained in the unstructured texts of Wikipedia. Our goal is to extract structured entity-relationships in RDF from such unstructured texts, ultimately using them to enrich existing data sources. Our extraction technique is aimed to be topic-independent, leveraging grammatical dependency of sentences and context semantic refinement. Preliminary evaluations of the proposed approach has shown some promising results."
research_127,127,"Dilshod Ibragimov, Katja Hose, Torben Bach Pedersen and Esteban Zimanyi",Ibragimov,Optimizing Aggregate SPARQL Queries using Materialized RDF Views,research,Querying,3,Dilshod Ibragimov,research_127.pdf,502,2016-10-19T14:40:00,2016-10-19T15:00:00,"During the past couple of years, more and more data has been published as native RDF datasets. In this setup, both the size of the datasets and the need to process aggregate queries represent challenges for standard SPARQL query processing techniques. To overcome these limitations, materialized views can be created and used as a source of precomputed partial results during query processing. However, materialized view techniques, as proposed in relational databases, do not support RDF specifics, such as incompleteness and the need to support implicit (derived) information. Therefore, to overcome these challenges, this paper proposes MARVEL – the approach consisting of a view selection algorithm based on an RDF-specific cost model, a view definition syntax. and an algorithm for rewriting SPARQL queries using materialized RDF views. The experimental evaluation shows that the approach can improve query response time by more than an order of magnitude and is able to efficiently handle RDF specifics."
research_120,120,"Xiaowang Zhang, Zhiyong Feng, Xin Wang, Guozheng Rao and Wenrui Wu",Zhang,Context-Free Path Queries on RDF Graphs,research,Querying,4,Xiaowang Zhang,research_120.pdf,502,2016-10-19T15:00:00,2016-10-19T15:20:00,"Navigational graph queries are an important class of queries that can extract implicit binary relations over the nodes of input graphs. Most of the navigational query languages used in the RDF community, e.g. property paths in W3C SPARQL 1.1 and nested regular expressions in nSPARQL, are based on the regular expressions. It is known that regular expressions have limited expressivity; for instance, some natural queries, like same generations-queries} are not expressible with regular expressions. To overcome this limitation, in this paper, we present cfSPARQL, an extension of SPARQL query language equipped with context-free grammars. The cfSPARQL language is strictly more expressive than property paths and nested expressions. The additional expressivity can be used for modelling graph similarities, graph summarization and ontology alignment. Despite the increasing expressivity, we show that cfSPARQL still enjoys a low computational complexity and can be evaluated efficiently."
demo_13,13,"Damien Graux, Pierre Geneves and Nabil Layaida",Graux,Smart Trip Alternatives for the Curious,demo,Poster and Demo session,83,Damien Graux,demo_13.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"When searching for flights, current systems often suggest routes involving waiting times at stopovers. There might exist alternative routes which are more attractive from a touristic perspective because their duration is not necessarily much longer while offering enough time in an appropriate place. Choosing among such alternatives requires additional planning efforts to make sure that e.g. points of interest can conveniently be reached in the allowed time frame. We present a system that automatically computes smart trip alternatives between any two cities. To do so, it searches points of interest in large semantic datasets considering the set of accessible areas around each possible layover. It then elects feasible alternatives and displays their differences with respect to the default trip."
demo_16,16,"Evgeny Kharlamov, Sebastian Brandt, Martin Giese, Ernesto Jimenez-Ruiz, Yannis Kotidis, Steffen Lamparter, Theofilos Mailis, Christian Neuenstadt, Özgür Lütfü Özcep, Christoph Pinkel, Ahmet Soylu, Christoforos Svingos, Dmitriy Zheleznyakov, Ian Horrocks, Yannis Ioannidis, Ralf Möller and Arild Waaler",Kharlamov,Scalable Semantic Access to Siemens Static and Streaming Distributed Data,demo,Poster and Demo session,84,Evgeny Kharlamov,demo_16.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Numerous analytical tasks in industry rely on data integration solutions since they require data from multiple static and streaming data sources. In the context of the Optique project we have investigated how Semantic Technologies can enhance data integration and thus facilitate further data analysis. We introduced the notion Ontology-Based Stream-Static Data Integration and developed the system Optique to put our ideas in practice. In this demo we will show how Optique can help in diagnostics of power generating turbines in Siemens Energy. For this purpose we prepared anonymised streaming and static data from 950 Siemens power generating turbines with more than 100,000 sensors and deployed Optique on distributed environments with 128 nodes. The demo attendees will be able to see do diagnostics of turbines by registering and monitoring continuous queries that combine streaming and static data; to test scalability of our devoted stream management system that is able to process up to 1024 concurrent complex diagnostic queries with a 10 TB/day throughput; and to deploy Optique over Siemens demo data using our devoted interactive system to create abstraction semantic layers over data sources."
demo_78,78,"Syed Muhammad Ali Hasnain, Qaiser Mehmood, Syeda Sana E Zainab and Aidan Hogan",Hasnain,SPORTAL: Searching for Public SPARQL Endpoints,demo,Poster and Demo session,85,Syed Muhammad Ali Hasnain,demo_78.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"There are hundreds of SPARQL endpoints on the Web, but finding an endpoint relevant to a client's needs is difficult: each endpoint acts like a black box, often without a description of its content. Herein we briefly describe SPORTAL: a system that collects meta-data about the content of endpoints and collects them into a central catalogue over which clients can search. SPORTAL sends queries to individual endpoints offline to learn about their content, generating a best-effort VoID description for each endpoint. These descriptions can then be searched and queried over by clients in the SPORTAL user interface, for example, to find endpoints that contain instances of a given class, or triples with a given predicate, or more complex requests such as endpoints with at least 1,000 images of people. Herein we give a brief overview of SPORTAL, its design and functionality, and the features that shall be demoed at the conference."
demo_79,79,"Gregoire Burel, Lara Piccolo and Harith Alani",Burel,"Monitoring, Discussing and Publishing Energy Consumption Data using EnergyUse",demo,Poster and Demo session,86,Gregoire Burel,demo_79.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We introduce EnergyUse, a collaborative website designed for raising climate change awareness by offering users the ability to view and compare the actual energy consumption of various appliances, and to share and discuss energy conservation tips in an open and social environment. The platform collects data from smart plugs, and exports appliance consumption and community generated energy tips as linked data. EnergyUse is supported by multiples automatic processes that semantically link related contributions, generate appliances descriptions and publish consumption data using the EnergyUse ontology."
research_49,49,"Fabian M. Suchanek, Colette Menard, Meghyn Bienvenu and Cyril Chapellier",Suchanek,Can you imagine... a language for combinatorial creativity?,research,Knowledge Representation,3,Fabian M. Suchanek,research_49.pdf,502,2016-10-19T11:40:00,2016-10-19T12:00:00," Combinatorial creativity combines existing concepts in a novel way in order to produce a new concept. For example, we can imag- ine jewelry that measures blood pressure. For this, we would combine the concept of jewelry with the capabilities of medical devices. Combinato- rial creativity can be used to develop new business ideas, to find plots for books or movies, or simply to disrupt conventional thinking. In this paper, we propose a formal language for combinatorial creativity, based on description logics. We show that our language can be used to model existing inventions and (to a limited degree) to generate new concepts.
"
demo_75,75,"Bernardo Cuenca Grau, Evgeny Kharlamov, Sarunas Marciuska, Dmitriy Zheleznyakov and Marcelo Arenas",Grau,SemFacet: Faceted Search over Ontology Enhanced Knowledge Graphs,demo,Poster and Demo session,87,Bernardo Cuenca Grau,demo_75.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this demo we present the SemFacet system for faceted search over ontology enhanced Knowledge Graphs (KGs) stored in RDF. SemFacet allows users to query KGs with relatively complex SPARQL queries via an intuitive Amazon-like interface. SemFacet can compute faceted interfaces over large scale RDF datasets by relying on incremental algorithms and over large ontologies by exploiting ontology projection techniques. SemFacet relies on an in-memory triple store and current implementation bundles JRDFox, Sesame, Stardog, and PAGOdA. During the demonstration the attendees can try SemFacet by exploring Yago KG."
demo_24,24,"Damian Bursztyn, Francois Goasdoue and Ioana Manolescu",Bursztyn,Optimizing FOL reducible query answering: understanding performance challenges,demo,Poster and Demo session,37,Damian Bursztyn,demo_24.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Semantic Web data management raises the challenge of answering queries under constraints (i.e., in the presence of implicit data). To bridge the gap between this extended setting and that of query evaluation provided by database engines, a reasoning step (w.r.t. the constraints) is necessary before query evaluation. A large and useful set of ontology languages enjoys FOL reducibility of query answering: queries can be answered by evaluating a SQLized first-order logic (FOL) formula (obtained from the query and the ontology) directly against the explicitly stored data (i.e., without considering the ontological constraints).
Our demonstration showcases to the attendees, and analyzes, the performance of several reformulation-based query answering techniques, including one we recently devised, applied to the lightweight description logic DL-LiteR underpinning the W3C’s OWL2 QL profile."
demo_71,71,"Raf Buyle, Pieter Colpaert, Mathias Van Compernolle, Peter Mechant, Veronique Volders, Ruben Verborgh and Erik Mannens",Buyle,Local Council Decisions as Linked Data: a proof of concept,demo,Poster and Demo session,89,Raf Buyle,demo_71.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Base registries are trusted authentic information sources controlled by an appointed public administration or organization appointed by the government. Maintaining a base registry comes with extra maintenance costs to create the dataset and keep it up to date. In this paper, we study the possibility to entangle the maintenance of base registries at the core of existing administrative processes and to reduce the cost of maintaining a new data source. We demonstrate a method to manage Local Council Decisions as Linked Data, which creates a new base registry for mandates. We found that no extra effort was needed in the process by local administrations. We show that an end-to-end approach for Local Council Decisions as Linked Data is feasible. Furthermore, using this proof of concept, we established a momentum to roll out these ideas for the region of Flanders in Belgium. "
demo_73,73,"Dmitriy Zheleznyakov, Evgeny Kharlamov, Vidar Klungre, Martin G. Skjæveland, Dag Hovland, Martin Giese, Ian Horrocks and Arild Waaler",Zheleznyakov,KeywDB: A System for Keyword-Driven Ontology-to-RDB Mapping Construction,demo,Poster and Demo session,90,Dmitriy Zheleznyakov,demo_73.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,In ontology-based data access (OBDA) the users access relational databases (RDBs) via ontologies that mediate between the users and the data. Ontologies are connected to data via declarative ontology-to-RDB mappings that relate each ontological term to an SQL query. In this demo we present our system KeywDB that facilitates construction of ontology-to-RDB mappings in an interactive fashion. In KeywDB users provide examples of entities for classes that require mappings and the system returnes a ranked list of such mappings. In doing so KeywDB relies on techniques for keyword query answering over RDBs. During the demo the attendees will try KeywDB with NorthWind and NPD FP databases and collections of mappings that we prepare.
poster_103,103,"Jongmin Lee, Youngkyoung Ham and Tony Lee",Lee,XB: A Large-scale Korean Knowledge Base for Question Answering Systems,poster,Poster and Demo session,91,Jongmin Lee,poster_103.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"There are many studies on question answering system which can answer to natural language questions. Diverse techniques are required for building this system, but it cannot be implemented without well-structured knowledge data. For this reason, we construct a large-scale knowledge base in Korean, with the goal of creating a uniquely Korean question answering system."
poster_101,101,"Nicolas Seydoux, Khalil Drira, Nathalie Hernandez and Thierry Monteil",Seydoux,Lowering knowledge : Making constrained devices semantically interoperable,poster,Poster and Demo session,92,Nicolas Seydoux,poster_101.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Semantic interoperability is an issue in heterogeneous IoT systems. The limited processing power and memory storage of constrained IoT nodes prevents them from handling enriched data. This paper proposes a method to lower complex knowledge representations into simpler structured data, based on the reuse of lifting mappings from data schemas to semantic models."
poster_100,100,"Femke Ongenae, Femke De Backere, Jelle Nelis, Stijn De Pestel, Christof Mahieu, Shirley Elprama, Charlotte Jewell, An Jacobs, Pieter Simoens and Filip De Turck",Ongenae,Personalized robot interactions to intercept behavioral disturbances of people with dementia,poster,Poster and Demo session,93,Femke Ongenae,poster_100.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"People with Dementia (PwD) exhibit Behavioral Disturbances (BD) that can be alleviated by personalized interactions, revisiting memories and promoting comfort and quality of life. However, caregivers are unable to spend a lot of time on these interactions. This work-in-progress poster details the design and deployment of a semantic Internet of Robotic Things (IoRT) platform that enables personalized interactions of a robot with a PwD to reduce and intercept BDs."
poster_107,107,"Lu Fang, Qingliang Miao and Yao Meng",Fang,DBpedia Entity Type Inference Using Categories,poster,Poster and Demo session,94,Lu Fang,poster_107.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"In this paper, we investigate how to identify entity type based on entity cat-egory information. In particular, we first calculate the statistical distribution of each category over all the types. And then we generate type candidates according to distribution probability. Finally we identify the correct type ac-cording to distribution probability, keywords in category and abstract. To evaluate the effectiveness of the approach, we conduct preliminary experi-ments on a real-world dataset from DBpedia. Experimental results indicate that our approach is effective in identifying entity types."
poster_106,106,"Takeshi Morita, Yu Sugawara, Ryota Nishimura and Takahira Yamaguchi",Morita,Implementing Customer Reception Service in Robot Cafe using Stream Reasoning and ROS based on PRINTEPS,poster,Poster and Demo session,95,Takeshi Morita,poster_106.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"We have developed PRactical INTElligent aPplicationS (PRINTEPS) which is a platform for developing comprehensive intelligence applications. This paper introduces an application of PRINTEPS for customer reception service in robot cafe by using stream reasoning and Robot Operating System (ROS) based on PRINTEPS, and for integrating image sensing with knowledge processing. Based on this platform, we demonstrate that the behaviors of a robot in a robot cafe can be modified by changing the applicable rule sets. "
poster_104,104,"Henning Agt-Rickauer, Jörg Waitelonis, Tabea Tietz and Harald Sack",Agt-Rickauer,Data Integration for the Media Value Chain,poster,Poster and Demo session,96,Henning Agt-Rickauer,poster_104.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"
With the switch from analog to digital technology the entire process of production, distribution, and archival of a film and tv program large amounts of data are created. Besides recorded and processed audiovisual information, in each single step of the production process and furthermore throughout the entire media value chain new metadata is created, administrated, and put into relation with already existing metadata mandatory for the management of these processes. Due to competing standards as well as to proprietary and incompatible interfaces of the applied software tools, a significant amount of this metadata is lost again and not available for subsequent steps in the process chain. As a consequence most of this valuable information has to be costly recreated in each single step of media production, distribution, and archival. Currently, there is no generally accepted nor commonly used metadata exchange format that is applied throughout the media value chain. But, also the market for media production companies has changed dramatically towards the internet as being the preferred distribution channel for all media content. Today’s available limited budget for media production companies puts additional pressure to work in a cost and time efficient way and not to waste resources due to the necessity of costly reengineering of lost metadata. The dwerft project aims to apply Linked Data principles for all metadata exchange through all steps of the media value chain. Starting with the very first idea for a script, all metadata are mapped to either existing or newly developed ontologies to be reused in subsequent steps of the media value chain. Thus, metadata collected during the media production becomes a valuable asset not only for each step from pre- to postproduction, but also in distribution and archival.
This paper presents results of the dwerft project about the successful integration of a set of film production tools based on the Linked Production Data Cloud, a technology platform for the film and tv industry to enable software interoperability used in production, distribution, and archival of audiovisual content."
doctoralconsortium_10,10,Miel Vander Sande,Sande,Studying Metadata for better client-server trade-offs in Linked Data publishing,doctoralconsortium,Session 2,1,Miel Vander Sande,doctoralconsortium_10.pdf,,2016-10-18T14:00:00,2016-10-18T14:15:00,"Client-server trade-offs can be analyzed using Linked Data Fragments,
which proposes an uniform view on all interfaces to rdf. This reveals a complete
spectrum between Linked Data documents and the sparql protocol, in which
we can advance the state-of-the-art of Linked Data publishing. This axis can be
explored in the following two dimensions: i) Selector, allowing different, more
complex questions for the server; and ii) Metadata, extending the response
with more information clients can use.
This work studies the second Metadata dimension in a practical Web context.
Considering the conditions on the Web, this problem becomes three-fold. First,
analog to the Web itself, ldf interfaces should exist in a distributed, scalable
manner in order to succeed. Generating additional metadata introduces overhead
on the server, which influences the ability to scale towards multiple clients. Second,
the communication between client and server uses the http protocol. Modeling,
serialization, and compression determine the extra load the overall network traffic.
Third, with query execution on the client, novel approaches need to apply this
metadata intelligently to increase efficiency.
Concretely, this work defines and evaluates a series of transparent, interchangeable,
and discoverable interface features. We proposed Triple Pattern Fragments, a Linked Data api with low-server cost, as a fundamental base . This
interface uses a single triple pattern as selector. To explore this research space,
we append this interface with different metadata, starting with an estimated
number of total matching triples. By combining several tpfs, sparql queries are
evaluated on the client-side, using the metadata for optimization. Hence, we can
measure the query execution"
poster_76,76,Octavian Rinciog and Vlad Posea,Rinciog,GovLOD: Towards to a Linked Open Data Portal,poster,Poster and Demo session,97,Octavian Rinciog,poster_76.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Nowadays, governments and public agencies publish open data at an exponentially growing rate on dedicated portals. These open data have a problem: they don’t have a well defined structure, because the focus is on publishing data and not on how they are used. GovLOD is a platform that aims to transform the information found in these heterogeneous files in Linked Open Data using RDF triples."
poster_70,70,"Ran Yu, Besnik Fetahu, Ujwal Gadiraju and Stefan Dietze",Yu,A Survey on Challenges in Web Markup Data for Entity Retrieval,poster,Poster and Demo session,98,Ran Yu,poster_70.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"Embedded markup based on Microdata, RDFa, and Microformats have become prevalent on the Web and constitute an unprecedented data source. RDF statements from markup are highly redundant, co-references are very frequent yet explicit links are missing, and with numerous errors in such statements.

We present a thorough analysis on the challenges associated with markup data in the context of entity retrieval. We analyze four main factors: (i) co-references, (ii) redundancy, (iii) inconsistencies, and (iv) accessibility of information in the case of URLs. We conclude with general guidelines on how to avoid such challenges when dealing with embedded markup data."
research_305,305,"Allan Third, George Gkotsis, Eleni Kaldoudi, George Drosatos, Nick Portokallidis, Stefanos Roumeliotis, Kalliopi Pafilis and John Domingue",Third,Integrating medical scientific knowledge with the semantically Quantified Self,research,Medical Applications,1,Allan Third,research_305.pdf,501,2016-10-21T13:30:00,2016-10-21T13:50:00,"The assessment of risk in medicine is a crucial task, depending on scientific knowledge derived by rigorous clinical studies regarding the (quantified) factors affecting biological changes, as well as on particular knowledge about the current status of a particular patient. Existing non-semantic risk prediction tools are typically based on hardcoded scientific knowledge, and only cover a very limited range of patient states. This makes them rapidly out of date, and limited in application, particularly for patients with co-morbidities (multiple co-occurring conditions). Semantic Web and Quantified Self technologies make it possible to address this task in a much more principled way, to maximise knowledge and data reuse and minimise maintenance requirements while enabling new and sophisticated applications involving widely-available biometric sensors. 

We present a framework for calculating clinical risk predictions for patients based on automatically-gathered biometric data. This framework relies on generic, reusable ontologies for representing clinical risk, and sensor readings, and reasoning to support the integration of data represented according to these ontologies. This integration makes novel use of Semantic Web technologies, and supports straightforward extension and maintenance by medical professionals. The framework is evaluated in terms of its predictions, extensibility and ease of use for domain experts."
poster_72,72,David Martin and Peter Patel-Schneider,Martin,EXISTStential Aspects of SPARQL,poster,Poster and Demo session,99,David Martin,poster_72.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The SPARQL 1.1 Query Language \cite{SPARQL} permits patterns inside {\sf FILTER}
expressions using the {\sf EXISTS} construct, specified by using substitution.
Substitution destroys some of the aspects of SPARQL that make
it suitable as a data access language. As well, substitution causes
problems in the SPARQL algebra and produces counterintuitive results.
Fixing the problems with {\sf EXISTS} is best done with a completely different
definition that does not use substitution at all."
research_210,210,"Steven de Rooij, Wouter Beek, Stefan Schlobach and Frank Van Harmelen",Rooij,Are Names Meaningful? Quantifying Social Meaning on the Semantic Web,research,Linked Data Measurement,1,Steven de Rooij,research_210.pdf,504+505,2016-10-20T13:30:00,2016-10-20T13:50:00,"According to Semantic Web standards, IRIs are individual
constants or predicate letters whose names are chosen arbitrarily
and carry no formal meaning. At the same time it is a well-known
aspect of Semantic Web pragmatics that IRIs are often constructed
mnemonically, in order to be meaningful to a human interpreter.
The latter has traditionally been termed 'Social Meaning', a
concept that has been discussed but not yet quantitatively
studied by the Semantic Web community.

In this paper we use statistical model learning as a method to
quantify the meaning that is (at least) encoded in Semantic Web
names, We implement the approach and evaluate it over hundreds of
thousands of data sets in order to illustrate its efficacy. Our
experiments confirm that many Semantic Web names are indeed
meaningful and, more interestingly, we provide a quantitative
lower bound on how much meaning is (at least) encoded in names on
a per-dataset basis.

To our knowledge, this is the first paper about the interaction
between social and formal meaning, as well as the first paper
that uses statistical model learning as a method to quantify
meaning in the Semantic Web context.
"
resource_37,37,"Stefano Faralli, Alexander Panchenko, Chris Biemann and Simone Paolo Ponzetto",Faralli,Linked Disambiguated Distributional Semantic Networks,resource,Natural Language Processing,1,Stefano Faralli,resource_37.pdf,504+505,2016-10-21T10:30:00,2016-10-21T10:50:00,"We present a new hybrid knowledge base that combines the contextual information of distributional models with the conciseness and precision of manually constructed lexical networks. In contrast to dense vector representations, our resource is human readable and interpretable, and can be easily embedded within the Semantic Web ecosystem. Manual evaluation based on human judgments and an extrinsic evaluation on the task of Word Sense Disambiguation both indicate the high quality of the resource, as well as the benefits of enriching top-down lexical knowledge resources with bottom-up distributional information from text."
doctoralconsortium_12,12,Joachim Van Herwegen,Herwegen,Querying Distributed Heterogeneous Linked Data Interfaces through Reasoning,doctoralconsortium,Session 3,4,Joachim Van Herwegen,doctoralconsortium_12.pdf,,2016-10-18T16:45:00,2016-10-18T17:00:00,"Linked Data can be distributed through multiple interfaces on the Web,
each of them with their own expressivity.
However, there is no generic client available that can handle querying over multiple interfaces.
This increases the complexity of combining datasets and designing new interfaces.
One can imagine the difficulties that arise 
when trying to create a client querying various interfaces at the same time, 
that can be discovered just in time.
To this end, I aim to design a generic Linked Data querying engine
capable of handling different interfaces that can easily be extended.
Rule-based reasoning is going to be explored
to combine different interfaces without intervention of a human developer.
Using an iterative approach to extend Linked Data interfaces,
I am going to evaluate different querying set-ups for the SPARQL language.
Preliminary results indicate a broad spectrum of yet to be explored options.
As the PhD is still in an early phase, we hope to narrow the scope in the next months,
based on feedback of the doctoral consortium."
research_303,303,Markus Krötzsch and Veronika Thost,Krötzsch,Ontologies for Knowledge Graphs: Breaking the Rules,research,Knowledge Representation,2,Markus Krötzsch,research_303.pdf,502,2016-10-19T11:20:00,2016-10-19T11:40:00,"Large-scale knowledge graphs (KGs) abound in industry and academia. 
They provide a unified format for integrating information sources, 
aided by standards such as, e.g., the W3C RDB to RDF Mapping Language. 
Meaningful semantic integration, however, is much harder than 
syntactic alignment. Ontologies could be an interoperable and 
declarative solution to this task. At a closer look, however, we find 
that popular ontology languages, such as OWL and Datalog, cannot 
express even the most basic relationships on the normalised data 
format of KGs. Existential rules are more powerful, but may make 
reasoning undecidable, and normalising them to suit KGs can destroy 
syntactic restrictions that ensure decidability and low complexity. We 
study this issue for several classes of existential rules and derive more 
general syntactic criteria to recognise well-behaved rule-based ontologies
over knowledge graphs."
demo_3,3,Ghislain Auguste Atemezing and Pierre-Yves Vandenbussche,Atemezing,QA4LOV: A Natural Language Interface to Linked Open Vocabulary,demo,Poster and Demo session,101,Ghislain Auguste Atemezing,demo_3.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"There is an increasing presence of structured data due to the adoption of Linked data principles on the web. At the same time, web users have different skills and want to be able to interact with Linked datasets in various manner, such as asking questions in natural language. Over the last years, the QALD challenges series are becoming the references for benchmarking question answering systems. However, QALD questions are targeted on datasets, not on vocabulary catalogues. This paper proposed a first implementation of Query Answering system (QA) applied to the Linked Open Vocabularies (LOV) catalogue, mainly focused on metadata information retrieval. The goal is to provide to end users yet another access to metadata information available in LOV using natural language questions."
demo_6,6,"Ruben Taelman, Pieter Heyvaert, Ruben Verborgh, Erik Mannens and Rik Van de Walle",Taelman,Querying Dynamic Datasources with Continuously Mapped Sensor Data,demo,Poster and Demo session,102,Ruben Taelman,demo_6.pdf,3rd Floor,2016-10-19T18:00:00,2016-10-19T21:00:00,"The world contains a large amount of sensors that produce new data at a high frequency. It is currently very hard to find public services that expose these measurements as dynamic Linked Data, We investigate how sensor data can be published continuously on the Web at a low cost. This paper describes how the publication of various sensor data sources can be done by continuously mapping raw sensor data to RDF and inserting it into a live, low-cost server. This makes it possible for clients to continuously evaluate dynamic queries using public sensor data. For our demonstration, we will illustrate how this pipeline works for the publication of temperature and humidity data originating from a microcontroller, and how it can be queried."
research_110,110,"Chetana Gavankar, Yuan-Fang Li and Ganesh Ramakrishnan",Gavankar,Explicit Query Interpretation and Diversification for Context-driven Concept Search across Ontologies,research,Search (I),1,Chetana Gavankar,research_110.pdf,502,2016-10-20T13:30:00,2016-10-20T13:50:00,"Finding relevant concepts from a corpus of ontologies is useful in many scenarios, including document classification, web page annotation, and automatic ontology population. Millions of concepts are contained in a large number of ontologies across diverse domains. SPARQL-based query demands knowledge of the structure of ontologies and the query language, whereas more user-friendly, simple keyword-based approaches suffer from false positives as concept descriptions in ontologies may be ambiguous and overlapping. In this paper, we propose a keyword-based concept search framework that (1) exploits the structure and semantics in ontologies, by constructing contexts for each concept; (2) generates the interpretations of a query; and (3) balances relevance and diversity of search results. A comprehensive evaluation against both the domain-specific BioPortal and the general-purpose Falcons on widely-used performance metrics demonstrates that our system outperforms both."
research_115,115,Melisachew Wudage Chekol and Giuseppe Pirrò,Chekol,Containment of Expressive SPARQL Navigational Queries,research,Querying,2,Melisachew Wudage Chekol,research_115.pdf,502,2016-10-19T14:20:00,2016-10-19T14:40:00,"Query containment is one of the building block of query optimization techniques. In the relational world, query containment is a well-studied problem. At the same time it is well-understood that relational queries are not enough to cope with graph-structured data, where one is interested in expressing queries that capture navigation in the graph. This paper contributes a study on the problem of query containment for an expressive class of navigational queries called Extended Property Paths (EPPs). EPPs are more expressive than previous navigational extensions of SPARQL like property paths and nested regular expressions, for which containment has already been studied. We attack the problem of EPPs (and SPARQL with EPPs) containment and provide complexity bounds.
"
research_119,119,"Shen Gao, Daniele Dell'Aglio, Soheila Dehghanzadeh, Abraham Bernstein, Emanuele Della Valle and Alessandra Mileo",Gao,Planning Ahead: Stream-Driven Linked-Data Access under Update-Budget Constraints,research,Streams,1,Shen Gao,research_119.pdf,501,2016-10-20T13:30:00,2016-10-20T13:50:00,"Data stream applications are becoming increasingly popular on the web. In these applications, one query pattern is especially prominent: a join between a continuous data stream and some background data (BGD). Oftentimes, the target BGD is large, maintained externally, changing slowly, and costly to query (both in terms of time and money). Hence, practical applications usually maintain a local (cached) view of the relevant BGD. Given that these caches are not updated as part of the transaction modifying the original BGD, they should be maintained under realistic budget constraints (in terms of latency, computation time, and possibly financial cost) to avoid stale data leading to wrong answers. 

This paper proposes to model the join between streams and the BGD as a bipartite graph. By exploiting the graph structure, we keep the quality of results good enough without refreshing the entire cache for each evaluation. We also introduce two extensions to this method: first, we consider both the sliding window (specifying the currently relevant section of the data stream) and the change rate of the BGD to focus on updates that have the longest effect. Second, by considering the future impact of a query to the BGD we propose to sometimes delay updates to provide more fresher answers in future.

Using an implemented system we empirically show that we can improve result freshness by 93% over baseline algorithms such as Random Selection or Least Recently Updated."
